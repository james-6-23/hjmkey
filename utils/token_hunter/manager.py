"""
GitHub Token ç®¡ç†å™¨
è´Ÿè´£tokençš„å­˜å‚¨ã€å¾ªç¯ä½¿ç”¨å’Œè‡ªåŠ¨ç®¡ç†
"""

import os
import json
import threading
from pathlib import Path
from typing import List, Dict, Optional, Any
from datetime import datetime
import logging

from .validator import TokenValidator, TokenValidationResult

logger = logging.getLogger(__name__)


class NoValidTokenError(Exception):
    """æ²¡æœ‰å¯ç”¨tokenå¼‚å¸¸"""
    pass


class NoQuotaError(Exception):
    """æ‰€æœ‰tokené¢åº¦è€—å°½å¼‚å¸¸"""
    pass


class TokenManager:
    """
    GitHub Token ç®¡ç†å™¨
    å®ç°tokençš„å¾ªç¯ä½¿ç”¨ã€è‡ªåŠ¨éªŒè¯å’Œå¤±æ•ˆç®¡ç†
    """
    
    def __init__(self, tokens_file: str = "data/github_tokens.txt", auto_validate: bool = True):
        """
        åˆå§‹åŒ–Tokenç®¡ç†å™¨
        
        Args:
            tokens_file: tokenså­˜å‚¨æ–‡ä»¶è·¯å¾„
            auto_validate: æ˜¯å¦è‡ªåŠ¨éªŒè¯æ–°æ·»åŠ çš„token
        """
        self.tokens_file = Path(tokens_file)
        self.auto_validate = auto_validate
        self.validator = TokenValidator()
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.tokens_file.parent.mkdir(parents=True, exist_ok=True)
        
        # Tokenåˆ—è¡¨å’ŒçŠ¶æ€
        self.tokens: List[str] = []
        self.token_stats: Dict[str, Dict[str, Any]] = {}
        self.current_index = 0
        self.lock = threading.Lock()
        
        # åŠ è½½tokens
        self._load_tokens()
        
        # ç»Ÿè®¡æ–‡ä»¶è·¯å¾„
        self.stats_file = self.tokens_file.parent / "token_stats.json"
        self._load_stats()
        
        # æ— æ•ˆtokensè®°å½•æ–‡ä»¶
        self.invalid_tokens_file = self.tokens_file.parent / "invalid_tokens.txt"
        
        logger.info(f"ğŸ“‚ Tokenç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ŒåŠ è½½äº† {len(self.tokens)} ä¸ªtokens")
    
    def _load_tokens(self) -> None:
        """ä»æ–‡ä»¶åŠ è½½tokens"""
        self.tokens = []
        
        if not self.tokens_file.exists():
            logger.warning(f"âš ï¸ Tokensæ–‡ä»¶ä¸å­˜åœ¨: {self.tokens_file}")
            # åˆ›å»ºç©ºæ–‡ä»¶
            self.tokens_file.touch()
            return
        
        try:
            with open(self.tokens_file, 'r', encoding='utf-8') as f:
                for line in f:
                    token = line.strip()
                    # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                    if token and not token.startswith('#'):
                        self.tokens.append(token)
            
            logger.info(f"âœ… ä» {self.tokens_file} åŠ è½½äº† {len(self.tokens)} ä¸ªtokens")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½tokenså¤±è´¥: {e}")
    
    def _save_tokens(self) -> None:
        """ä¿å­˜tokensåˆ°æ–‡ä»¶"""
        try:
            with open(self.tokens_file, 'w', encoding='utf-8') as f:
                f.write("# GitHub Tokens åˆ—è¡¨\n")
                f.write(f"# æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# æ€»æ•°: {len(self.tokens)}\n\n")
                
                for token in self.tokens:
                    f.write(f"{token}\n")
            
            logger.info(f"ğŸ’¾ ä¿å­˜äº† {len(self.tokens)} ä¸ªtokensåˆ° {self.tokens_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜tokenså¤±è´¥: {e}")
    
    def _load_stats(self) -> None:
        """åŠ è½½tokenç»Ÿè®¡ä¿¡æ¯"""
        if self.stats_file.exists():
            try:
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    self.token_stats = json.load(f)
            except Exception as e:
                logger.error(f"âŒ åŠ è½½ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
                self.token_stats = {}
        else:
            self.token_stats = {}
    
    def _save_stats(self) -> None:
        """ä¿å­˜tokenç»Ÿè®¡ä¿¡æ¯"""
        try:
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.token_stats, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
    
    def _update_token_stats(self, token: str, success: bool = True) -> None:
        """æ›´æ–°tokenä½¿ç”¨ç»Ÿè®¡"""
        token_key = token[:10] + "..." if len(token) > 10 else token
        
        if token_key not in self.token_stats:
            self.token_stats[token_key] = {
                "first_used": datetime.now().isoformat(),
                "last_used": datetime.now().isoformat(),
                "use_count": 0,
                "success_count": 0,
                "fail_count": 0
            }
        
        stats = self.token_stats[token_key]
        stats["last_used"] = datetime.now().isoformat()
        stats["use_count"] += 1
        
        if success:
            stats["success_count"] += 1
        else:
            stats["fail_count"] += 1
        
        # å®šæœŸä¿å­˜ç»Ÿè®¡
        if stats["use_count"] % 10 == 0:
            self._save_stats()
    
    def get_next_token(self) -> str:
        """
        è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„tokenï¼ˆå¾ªç¯ä½¿ç”¨ï¼‰
        
        Returns:
            å¯ç”¨çš„GitHub token
            
        Raises:
            NoValidTokenError: æ²¡æœ‰å¯ç”¨çš„token
            NoQuotaError: æ‰€æœ‰tokené¢åº¦è€—å°½
        """
        with self.lock:
            if not self.tokens:
                raise NoValidTokenError("âŒ æ²¡æœ‰å¯ç”¨çš„GitHub tokensï¼Œè¯·å…ˆæ·»åŠ tokens")
            
            # è®°å½•å°è¯•æ¬¡æ•°ï¼Œé¿å…æ— é™å¾ªç¯
            attempts = 0
            tokens_with_quota = []
            
            while attempts < len(self.tokens):
                token = self.tokens[self.current_index]
                self.current_index = (self.current_index + 1) % len(self.tokens)
                attempts += 1
                
                # æ£€æŸ¥tokené¢åº¦
                logger.debug(f"ğŸ” æ£€æŸ¥tokené¢åº¦: {token[:10]}...")
                rate_limit = self.validator.check_rate_limit(token)
                
                if rate_limit:
                    if rate_limit.remaining > 0:
                        logger.info(f"âœ… ä½¿ç”¨token: {token[:10]}..., å‰©ä½™é¢åº¦: {rate_limit.remaining}/{rate_limit.limit}")
                        self._update_token_stats(token, success=True)
                        return token
                    else:
                        logger.warning(f"âš ï¸ Tokené¢åº¦è€—å°½: {token[:10]}..., é‡ç½®æ—¶é—´: {rate_limit.reset.strftime('%Y-%m-%d %H:%M:%S')}")
                        tokens_with_quota.append((token, rate_limit.reset))
                else:
                    # Tokenå¯èƒ½æ— æ•ˆï¼Œæ ‡è®°å¹¶ç»§ç»­
                    logger.warning(f"âš ï¸ Tokenå¯èƒ½æ— æ•ˆ: {token[:10]}...")
                    self._update_token_stats(token, success=False)
            
            # æ‰€æœ‰tokenéƒ½æ²¡æœ‰é¢åº¦
            if tokens_with_quota:
                # æ‰¾å‡ºæœ€æ—©æ¢å¤çš„token
                earliest_reset = min(tokens_with_quota, key=lambda x: x[1])
                reset_time = earliest_reset[1].strftime('%Y-%m-%d %H:%M:%S')
                raise NoQuotaError(f"âŒ æ‰€æœ‰ {len(self.tokens)} ä¸ªtokensé¢åº¦å·²è€—å°½ï¼Œæœ€æ—©å°†åœ¨ {reset_time} æ¢å¤")
            else:
                raise NoValidTokenError(f"âŒ æ‰€æœ‰ {len(self.tokens)} ä¸ªtokenséƒ½æ— æ•ˆæˆ–æ— æ³•è®¿é—®")
    
    def add_token(self, token: str, validate: bool = None) -> bool:
        """
        æ·»åŠ æ–°token
        
        Args:
            token: GitHub token
            validate: æ˜¯å¦éªŒè¯tokenï¼ˆNoneæ—¶ä½¿ç”¨é»˜è®¤è®¾ç½®ï¼‰
            
        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        with self.lock:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if token in self.tokens:
                logger.info(f"â„¹ï¸ Tokenå·²å­˜åœ¨: {token[:10]}...")
                return False
            
            # æ˜¯å¦éœ€è¦éªŒè¯
            should_validate = validate if validate is not None else self.auto_validate
            
            if should_validate:
                logger.info(f"ğŸ” éªŒè¯æ–°token: {token[:10]}...")
                result = self.validator.validate(token)
                
                if not result.valid:
                    logger.warning(f"âŒ TokenéªŒè¯å¤±è´¥: {result.reason}")
                    # è®°å½•æ— æ•ˆtoken
                    self._record_invalid_token(token, result.reason)
                    return False
                
                logger.info(f"âœ… TokenéªŒè¯æˆåŠŸ: ç”¨æˆ·={result.user}, é¢åº¦={result.rate_limit.remaining if result.rate_limit else 'unknown'}")
            
            # æ·»åŠ token
            self.tokens.append(token)
            self._save_tokens()
            
            # åˆå§‹åŒ–ç»Ÿè®¡
            self._update_token_stats(token, success=True)
            
            logger.info(f"âœ… æˆåŠŸæ·»åŠ æ–°tokenï¼Œå½“å‰å…± {len(self.tokens)} ä¸ªtokens")
            return True
    
    def add_tokens_batch(self, tokens: List[str], validate: bool = None) -> Dict[str, bool]:
        """
        æ‰¹é‡æ·»åŠ tokens
        
        Args:
            tokens: tokenåˆ—è¡¨
            validate: æ˜¯å¦éªŒè¯
            
        Returns:
            æ·»åŠ ç»“æœå­—å…¸ {token: success}
        """
        results = {}
        
        logger.info(f"ğŸ“¦ æ‰¹é‡æ·»åŠ  {len(tokens)} ä¸ªtokens...")
        
        for token in tokens:
            results[token[:10] + "..."] = self.add_token(token, validate)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for v in results.values() if v)
        logger.info(f"âœ… æ‰¹é‡æ·»åŠ å®Œæˆ: {success_count}/{len(tokens)} æˆåŠŸ")
        
        return results
    
    def remove_token(self, token: str) -> bool:
        """
        ç§»é™¤token
        
        Args:
            token: è¦ç§»é™¤çš„token
            
        Returns:
            æ˜¯å¦ç§»é™¤æˆåŠŸ
        """
        with self.lock:
            if token in self.tokens:
                self.tokens.remove(token)
                self._save_tokens()
                
                # è®°å½•ä¸ºæ— æ•ˆtoken
                self._record_invalid_token(token, "æ‰‹åŠ¨ç§»é™¤")
                
                logger.info(f"ğŸ—‘ï¸ ç§»é™¤token: {token[:10]}..., å‰©ä½™ {len(self.tokens)} ä¸ªtokens")
                return True
            
            logger.warning(f"âš ï¸ Tokenä¸å­˜åœ¨: {token[:10]}...")
            return False
    
    def _record_invalid_token(self, token: str, reason: str) -> None:
        """è®°å½•æ— æ•ˆtoken"""
        try:
            with open(self.invalid_tokens_file, 'a', encoding='utf-8') as f:
                f.write(f"{datetime.now().isoformat()} | {token[:10]}... | {reason}\n")
        except Exception as e:
            logger.error(f"è®°å½•æ— æ•ˆtokenå¤±è´¥: {e}")
    
    def validate_all_tokens(self) -> Dict[str, TokenValidationResult]:
        """
        éªŒè¯æ‰€æœ‰tokens
        
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        logger.info(f"ğŸ” å¼€å§‹éªŒè¯æ‰€æœ‰ {len(self.tokens)} ä¸ªtokens...")
        
        results = {}
        invalid_tokens = []
        
        for token in self.tokens:
            result = self.validator.validate(token)
            results[token[:10] + "..."] = result
            
            if not result.valid:
                invalid_tokens.append(token)
        
        # ç§»é™¤æ— æ•ˆtokens
        if invalid_tokens:
            logger.warning(f"âš ï¸ å‘ç° {len(invalid_tokens)} ä¸ªæ— æ•ˆtokensï¼Œå°†è‡ªåŠ¨ç§»é™¤")
            for token in invalid_tokens:
                self.remove_token(token)
        
        # ç»Ÿè®¡
        valid_count = sum(1 for r in results.values() if r.valid)
        logger.info(f"âœ… éªŒè¯å®Œæˆ: {valid_count}/{len(results)} ä¸ªæœ‰æ•ˆtokens")
        
        return results
    
    def get_status(self) -> Dict[str, Any]:
        """
        è·å–ç®¡ç†å™¨çŠ¶æ€
        
        Returns:
            çŠ¶æ€ä¿¡æ¯å­—å…¸
        """
        status = {
            "total_tokens": len(self.tokens),
            "current_index": self.current_index,
            "tokens_file": str(self.tokens_file),
            "auto_validate": self.auto_validate,
            "stats": {}
        }
        
        # æ£€æŸ¥æ¯ä¸ªtokençš„é¢åº¦
        for i, token in enumerate(self.tokens):
            rate_limit = self.validator.check_rate_limit(token)
            token_key = f"token_{i+1}"
            
            if rate_limit:
                status["stats"][token_key] = {
                    "remaining": rate_limit.remaining,
                    "limit": rate_limit.limit,
                    "usage": f"{rate_limit.usage_percentage:.1f}%",
                    "reset": rate_limit.reset.strftime('%Y-%m-%d %H:%M:%S')
                }
            else:
                status["stats"][token_key] = {"status": "unknown"}
        
        return status
    
    def rotate_token(self) -> str:
        """
        å¼ºåˆ¶è½®æ¢åˆ°ä¸‹ä¸€ä¸ªtoken
        
        Returns:
            ä¸‹ä¸€ä¸ªtoken
        """
        with self.lock:
            if not self.tokens:
                raise NoValidTokenError("æ²¡æœ‰å¯ç”¨çš„tokens")
            
            self.current_index = (self.current_index + 1) % len(self.tokens)
            token = self.tokens[self.current_index]
            
            logger.info(f"ğŸ”„ è½®æ¢åˆ°token: {token[:10]}... (ç´¢å¼•: {self.current_index})")
            return token
    
    def clear_all_tokens(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰tokensï¼ˆè°¨æ…ä½¿ç”¨ï¼‰"""
        with self.lock:
            count = len(self.tokens)
            self.tokens = []
            self.current_index = 0
            self._save_tokens()
            
            logger.warning(f"ğŸ—‘ï¸ å·²æ¸…ç©ºæ‰€æœ‰ {count} ä¸ªtokens")


def main():
    """æµ‹è¯•å‡½æ•°"""
    # åˆ›å»ºç®¡ç†å™¨
    manager = TokenManager("test_tokens.txt")
    
    # æ·»åŠ æµ‹è¯•token
    test_token = os.getenv("GITHUB_TOKEN")
    if test_token:
        success = manager.add_token(test_token)
        print(f"æ·»åŠ token: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    # è·å–çŠ¶æ€
    status = manager.get_status()
    print(f"ç®¡ç†å™¨çŠ¶æ€: {json.dumps(status, indent=2, ensure_ascii=False)}")
    
    # è·å–ä¸‹ä¸€ä¸ªtoken
    try:
        token = manager.get_next_token()
        print(f"è·å–åˆ°token: {token[:10]}...")
    except (NoValidTokenError, NoQuotaError) as e:
        print(f"é”™è¯¯: {e}")


if __name__ == "__main__":
    main()