import base64
import random
import time
import logging
from typing import Dict, List, Optional, Any

import requests

# ä½¿ç”¨æ ‡å‡†loggingè€Œä¸æ˜¯è‡ªå®šä¹‰Logger
logger = logging.getLogger(__name__)


class GitHubClient:
    GITHUB_API_URL = "https://api.github.com/search/code"

    def __init__(self, tokens: List[str]):
        self.tokens = [token.strip() for token in tokens if token.strip()]
        self._token_ptr = 0
        # æ·»åŠ  token çŠ¶æ€è·Ÿè¸ª
        self._token_status = {token: {'remaining': 30, 'reset_time': 0, 'failed_count': 0}
                              for token in self.tokens}
        self._exhausted_tokens = set()  # å·²è€—å°½çš„ token

    def _next_token(self) -> Optional[str]:
        """æ™ºèƒ½é€‰æ‹©ä¸‹ä¸€ä¸ªå¯ç”¨çš„ token"""
        if not self.tokens:
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ token çš„é™æµæ—¶é—´å·²è¿‡
        current_time = time.time()
        for token in list(self._exhausted_tokens):
            if self._token_status[token]['reset_time'] < current_time:
                self._exhausted_tokens.remove(token)
                self._token_status[token]['remaining'] = 30  # é‡ç½®é…é¢
                logger.info(f"â™»ï¸ Token recovered from rate limit: {token[:20]}...")
        
        # è·å–å¯ç”¨çš„ tokensï¼ˆæœªè€—å°½çš„ï¼‰
        available_tokens = [t for t in self.tokens if t not in self._exhausted_tokens]
        
        if not available_tokens:
            # å¦‚æœæ‰€æœ‰ token éƒ½è€—å°½äº†ï¼Œç­‰å¾…æœ€æ—©æ¢å¤çš„ token
            if self._exhausted_tokens:
                min_reset = min(self._token_status[t]['reset_time'] for t in self._exhausted_tokens)
                wait_time = max(0, min_reset - current_time)
                if wait_time > 0:
                    logger.warning(f"â° All tokens exhausted, waiting {wait_time:.1f}s for recovery...")
                    time.sleep(wait_time + 1)
                    return self._next_token()  # é€’å½’è°ƒç”¨
            return None
        
        # é€‰æ‹©å‰©ä½™é…é¢æœ€å¤šçš„ token
        best_token = max(available_tokens,
                        key=lambda t: self._token_status[t]['remaining'])
        
        return best_token.strip() if isinstance(best_token, str) else best_token
    
    def _update_token_status(self, token: str, response: requests.Response):
        """æ›´æ–° token çš„é™æµçŠ¶æ€"""
        if not token or token not in self._token_status:
            return
            
        remaining = response.headers.get('X-RateLimit-Remaining')
        reset_time = response.headers.get('X-RateLimit-Reset')
        
        if remaining is not None:
            self._token_status[token]['remaining'] = int(remaining)
            
        if reset_time is not None:
            self._token_status[token]['reset_time'] = int(reset_time)
        
        # å¦‚æœé…é¢è€—å°½ï¼Œæ ‡è®°ä¸ºå·²è€—å°½
        if remaining is not None and int(remaining) == 0:
            self._exhausted_tokens.add(token)
            logger.warning(f"ğŸš« Token exhausted: {token[:20]}... (resets at {time.strftime('%H:%M:%S', time.localtime(int(reset_time)))})")

    def search_for_keys(self, query: str, max_retries: int = 5) -> Dict[str, Any]:
        all_items = []
        total_count = 0
        expected_total = None
        pages_processed = 0
        
        # æ·»åŠ å¤±è´¥é¡µé¢é‡è¯•é˜Ÿåˆ—
        failed_pages = []
        max_page_retries = 2  # æ¯ä¸ªå¤±è´¥é¡µé¢æœ€å¤šé‡è¯•2æ¬¡

        # ç»Ÿè®¡ä¿¡æ¯
        total_requests = 0
        failed_requests = 0
        rate_limit_hits = 0

        for page in range(1, 11):
            page_result = None
            page_success = False

            for attempt in range(1, max_retries + 1):
                current_token = self._next_token()

                headers = {
                    "Accept": "application/vnd.github.v3+json",
                    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
                }

                if current_token:
                    current_token = current_token.strip()
                    headers["Authorization"] = f"token {current_token}"

                params = {
                    "q": query,
                    "per_page": 100,
                    "page": page
                }

                try:
                    total_requests += 1
                    # æš‚æ—¶ä¸ä½¿ç”¨ä»£ç†ï¼Œç®€åŒ–å®ç°
                    response = requests.get(self.GITHUB_API_URL, headers=headers, params=params, timeout=30)
                    
                    # æ›´æ–° token çŠ¶æ€
                    if current_token:
                        self._update_token_status(current_token, response)
                    
                    rate_limit_remaining = response.headers.get('X-RateLimit-Remaining')
                    # åªåœ¨å‰©ä½™æ¬¡æ•°å¾ˆå°‘æ—¶è­¦å‘Š
                    if rate_limit_remaining and int(rate_limit_remaining) < 3:
                        logger.warning(f"âš ï¸ Rate limit low: {rate_limit_remaining} remaining, token: {current_token[:20]}...")
                    
                    response.raise_for_status()
                    page_result = response.json()
                    page_success = True
                    break

                except requests.exceptions.HTTPError as e:
                    status = e.response.status_code if e.response else None
                    failed_requests += 1
                    
                    if status in (403, 429):
                        rate_limit_hits += 1
                        
                        # æ ‡è®°å½“å‰ token ä¸ºè€—å°½
                        if current_token:
                            self._exhausted_tokens.add(current_token)
                            # ä»å“åº”å¤´è·å–é‡ç½®æ—¶é—´
                            reset_time = e.response.headers.get('X-RateLimit-Reset') if e.response else None
                            if reset_time:
                                self._token_status[current_token]['reset_time'] = int(reset_time)
                                wait_until_reset = int(reset_time) - time.time()
                                if wait_until_reset > 0:
                                    logger.info(f"â±ï¸ Token {current_token[:20]}... will reset in {wait_until_reset:.0f}s")
                        
                        # å¦‚æœè¿˜æœ‰å…¶ä»–å°è¯•æœºä¼šï¼Œå¿«é€Ÿåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª token
                        if attempt < max_retries:
                            wait = min(1 + random.uniform(0, 0.5), 5)  # å‡å°‘ç­‰å¾…æ—¶é—´
                            logger.debug(f"ğŸ”„ Switching token after rate limit (attempt {attempt}/{max_retries})")
                            time.sleep(wait)
                            continue
                        else:
                            logger.error(f"âŒ All retries exhausted for page {page}")
                    else:
                        # å…¶ä»–HTTPé”™è¯¯ï¼Œåªåœ¨æœ€åä¸€æ¬¡å°è¯•æ—¶è®°å½•
                        if attempt == max_retries:
                            logger.error(f"âŒ HTTP {status} error after {max_retries} attempts on page {page}")
                        time.sleep(min(2 ** attempt, 10))
                        continue

                except requests.exceptions.RequestException as e:
                    failed_requests += 1
                    wait = min(2 ** attempt, 30)

                    # åªåœ¨æœ€åä¸€æ¬¡å°è¯•æ—¶è®°å½•ç½‘ç»œé”™è¯¯
                    if attempt == max_retries:
                        logger.error(f"âŒ Network error after {max_retries} attempts on page {page}: {type(e).__name__}")

                    time.sleep(wait)
                    continue

            if not page_success or not page_result:
                if page == 1:
                    # ç¬¬ä¸€é¡µå¤±è´¥æ˜¯ä¸¥é‡é—®é¢˜
                    logger.error(f"âŒ First page failed for query: {query[:50]}...")
                    break
                else:
                    # å°†å¤±è´¥çš„é¡µé¢åŠ å…¥é‡è¯•é˜Ÿåˆ—
                    failed_pages.append({'page': page, 'retry_count': 0})
                    logger.debug(f"ğŸ“ Page {page} added to retry queue")
                continue

            pages_processed += 1

            if page == 1:
                total_count = page_result.get("total_count", 0)
                expected_total = min(total_count, 1000)

            items = page_result.get("items", [])
            current_page_count = len(items)

            if current_page_count == 0:
                if expected_total and len(all_items) < expected_total:
                    continue
                else:
                    break

            all_items.extend(items)

            if expected_total and len(all_items) >= expected_total:
                break

            if page < 10:
                sleep_time = random.uniform(0.5, 1.5)
                logger.info(f"â³ Processing query: ã€{query}ã€‘,page {page},item count: {current_page_count},expected total: {expected_total},total count: {total_count},random sleep: {sleep_time:.1f}s")
                time.sleep(sleep_time)

        # é‡è¯•å¤±è´¥çš„é¡µé¢
        if failed_pages and expected_total:
            logger.info(f"ğŸ”„ Retrying {len(failed_pages)} failed pages...")
            
            for failed_page_info in failed_pages[:]:  # ä½¿ç”¨åˆ‡ç‰‡åˆ›å»ºå‰¯æœ¬
                if failed_page_info['retry_count'] >= max_page_retries:
                    continue
                    
                page = failed_page_info['page']
                failed_page_info['retry_count'] += 1
                
                # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
                time.sleep(2 * failed_page_info['retry_count'])
                
                for attempt in range(1, 3):  # ç®€åŒ–çš„é‡è¯•é€»è¾‘
                    current_token = self._next_token()
                    if not current_token:
                        break
                        
                    headers = {
                        "Accept": "application/vnd.github.v3+json",
                        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
                    }
                    headers["Authorization"] = f"token {current_token}"
                    
                    params = {
                        "q": query,
                        "per_page": 100,
                        "page": page
                    }
                    
                    try:
                        response = requests.get(self.GITHUB_API_URL, headers=headers, params=params, timeout=30)
                        if current_token:
                            self._update_token_status(current_token, response)
                        response.raise_for_status()
                        
                        retry_result = response.json()
                        items = retry_result.get("items", [])
                        if items:
                            all_items.extend(items)
                            pages_processed += 1
                            failed_pages.remove(failed_page_info)
                            logger.info(f"âœ… Successfully recovered page {page} with {len(items)} items")
                        break
                        
                    except Exception as e:
                        if attempt == 2:
                            logger.debug(f"âš ï¸ Failed to recover page {page}: {type(e).__name__}")
                        continue

        final_count = len(all_items)

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if expected_total and final_count < expected_total:
            discrepancy = expected_total - final_count
            completeness = (final_count / expected_total) * 100
            
            if discrepancy > expected_total * 0.1:  # è¶…è¿‡10%æ•°æ®ä¸¢å¤±
                logger.warning(f"âš ï¸ Data completeness: {completeness:.1f}% ({final_count}/{expected_total} items retrieved)")
                
                # å¦‚æœæ•°æ®å®Œæ•´æ€§ä½äº50%ï¼Œè®°å½•æ›´ä¸¥é‡çš„è­¦å‘Š
                if completeness < 50:
                    logger.error(f"âŒ Severe data loss: Only {completeness:.1f}% of expected data retrieved")
            else:
                logger.info(f"âœ… Good data completeness: {completeness:.1f}%")

        # ä¸»è¦æˆåŠŸæ—¥å¿— - ä¸€æ¡æ—¥å¿—åŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯
        failed_pages_count = len(failed_pages)
        logger.info(f"ğŸ” GitHub search complete: query:ã€{query}ã€‘ | pages:{pages_processed}/{10} | items:{final_count}/{expected_total or '?'} | failed pages:{failed_pages_count}")

        result = {
            "total_count": total_count,
            "incomplete_results": final_count < expected_total if expected_total else False,
            "items": all_items
        }

        return result

    def get_file_content(self, item: Dict[str, Any]) -> Optional[str]:
        repo_full_name = item["repository"]["full_name"]
        file_path = item["path"]

        metadata_url = f"https://api.github.com/repos/{repo_full_name}/contents/{file_path}"
        headers = {
            "Accept": "application/vnd.github.v3+json",
        }

        token = self._next_token()
        if token:
            headers["Authorization"] = f"token {token}"

        try:
            logger.info(f"ğŸ” Processing file: {metadata_url}")
            # æš‚æ—¶ä¸ä½¿ç”¨ä»£ç†ï¼Œç®€åŒ–å®ç°
            metadata_response = requests.get(metadata_url, headers=headers, timeout=15)
            
            # æ›´æ–° token çŠ¶æ€
            if token:
                self._update_token_status(token, metadata_response)
            
            metadata_response.raise_for_status()
            file_metadata = metadata_response.json()

            # æ£€æŸ¥æ˜¯å¦æœ‰base64ç¼–ç çš„å†…å®¹
            encoding = file_metadata.get("encoding")
            content = file_metadata.get("content")
            
            if encoding == "base64" and content:
                try:
                    # è§£ç base64å†…å®¹
                    decoded_content = base64.b64decode(content).decode('utf-8')
                    return decoded_content
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to decode base64 content: {e}, falling back to download_url")
            
            # å¦‚æœæ²¡æœ‰base64å†…å®¹æˆ–è§£ç å¤±è´¥ï¼Œä½¿ç”¨åŸæœ‰çš„download_urlé€»è¾‘
            download_url = file_metadata.get("download_url")
            if not download_url:
                logger.warning(f"âš ï¸ No download URL found for file: {metadata_url}")
                return None

            # æš‚æ—¶ä¸ä½¿ç”¨ä»£ç†ï¼Œç®€åŒ–å®ç°
            content_response = requests.get(download_url, headers=headers)
            logger.info(f"â³ checking for keys from:  {download_url},status: {content_response.status_code}")
            content_response.raise_for_status()
            return content_response.text

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Failed to fetch file content: {metadata_url}, {type(e).__name__}")
            return None

    @staticmethod
    def create_instance(tokens: List[str]) -> 'GitHubClient':
        return GitHubClient(tokens)
