"""
GitHub å®¢æˆ·ç«¯ V2 - é›†æˆ TokenPool çš„å¢å¼ºç‰ˆ
"""

import os
import base64
import random
import time
import logging
from typing import Dict, List, Optional, Any, Tuple
import requests
from utils.token_pool import TokenPool, TokenSelectionStrategy
from utils.security_utils import mask_key

logger = logging.getLogger(__name__)


class GitHubClientV2:
    """å¢å¼ºç‰ˆ GitHub å®¢æˆ·ç«¯ï¼Œé›†æˆ TokenPool"""
    
    GITHUB_API_URL = "https://api.github.com/search/code"
    
    def __init__(self, token_pool: TokenPool, proxy_config: Optional[Dict[str, str]] = None):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            token_pool: TokenPool å®ä¾‹
            proxy_config: ä»£ç†é…ç½®å­—å…¸ (å¯é€‰)
        """
        self.token_pool = token_pool
        self.session = requests.Session()
        
        # é…ç½®ä»£ç†
        self.proxy_config = proxy_config or self._get_proxy_from_env()
        if self.proxy_config:
            self.session.proxies.update(self.proxy_config)
            proxy_url = self.proxy_config.get('http') or self.proxy_config.get('https')
            logger.info(f"ğŸŒ Using proxy: {proxy_url}")
        
        # ç»Ÿè®¡
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        
        logger.info(f"ğŸŒ GitHub Client V2 initialized with token pool")
    
    def _get_proxy_from_env(self) -> Optional[Dict[str, str]]:
        """
        ä»ç¯å¢ƒå˜é‡è·å–ä»£ç†é…ç½®
        
        Returns:
            ä»£ç†é…ç½®å­—å…¸æˆ– None
        """
        proxy_config = {}
        
        # æ£€æŸ¥ HTTP_PROXY å’Œ HTTPS_PROXY
        http_proxy = os.getenv('HTTP_PROXY') or os.getenv('http_proxy')
        https_proxy = os.getenv('HTTPS_PROXY') or os.getenv('https_proxy')
        
        if http_proxy:
            proxy_config['http'] = http_proxy
            logger.debug(f"Found HTTP proxy: {http_proxy}")
        
        if https_proxy:
            proxy_config['https'] = https_proxy
            logger.debug(f"Found HTTPS proxy: {https_proxy}")
        
        # æ£€æŸ¥ NO_PROXY
        no_proxy = os.getenv('NO_PROXY') or os.getenv('no_proxy')
        if no_proxy:
            # requests åº“ä¼šè‡ªåŠ¨å¤„ç† NO_PROXY
            os.environ['NO_PROXY'] = no_proxy
            logger.debug(f"NO_PROXY set: {no_proxy}")
        
        return proxy_config if proxy_config else None
    
    def search_for_keys(self, query: str, max_retries: int = 5) -> Dict[str, Any]:
        """
        æœç´¢å¯†é’¥ï¼ˆæ”¹è¿›ç‰ˆï¼šå¢å¼ºæ•°æ®å®Œæ•´æ€§ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            æœç´¢ç»“æœ
        """
        all_items = []
        total_count = 0
        expected_total = None
        pages_processed = 0
        
        # æ”¹è¿›ï¼šå¢åŠ å¤±è´¥é¡µé¢é‡è¯•æœºåˆ¶
        failed_pages = []
        max_page_retries = 3  # å¢åŠ é‡è¯•æ¬¡æ•°
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_requests = 0
        failed_requests = 0
        rate_limit_hits = 0
        successful_pages = set()  # è®°å½•æˆåŠŸçš„é¡µé¢
        
        # æ”¹è¿›ï¼šåŠ¨æ€è®¡ç®—éœ€è¦è·å–çš„é¡µæ•°
        max_pages = 10  # GitHub APIé™åˆ¶æœ€å¤š1000ä¸ªç»“æœï¼ˆ100ä¸ª/é¡µ * 10é¡µï¼‰
        
        for page in range(1, max_pages + 1):
            page_result = None
            page_success = False
            
            for attempt in range(1, max_retries + 1):
                # ä» TokenPool è·å–ä»¤ç‰Œ
                token = self.token_pool.select_token()
                if not token:
                    logger.error("âŒ No available tokens in pool")
                    time.sleep(5)  # ç­‰å¾…ä»¤ç‰Œæ¢å¤
                    continue
                
                # æ„å»ºè¯·æ±‚
                headers = {
                    "Accept": "application/vnd.github.v3+json",
                    "User-Agent": "Mozilla/5.0 (compatible; HajimiKing/2.0)",
                    "Authorization": f"token {token}"
                }
                
                params = {
                    "q": query,
                    "per_page": 100,
                    "page": page
                }
                
                try:
                    total_requests += 1
                    self.total_requests += 1
                    
                    # å‘é€è¯·æ±‚ï¼ˆä½¿ç”¨é…ç½®çš„ä»£ç†ï¼‰
                    start_time = time.time()
                    response = self.session.get(
                        self.GITHUB_API_URL,
                        headers=headers,
                        params=params,
                        timeout=30,
                        proxies=self.proxy_config  # æ˜¾å¼ä¼ é€’ä»£ç†é…ç½®
                    )
                    response_time = time.time() - start_time
                    
                    # æ›´æ–° TokenPool
                    self.token_pool.update_token_status(token, {
                        'status_code': response.status_code,
                        'headers': dict(response.headers),
                        'response_time': response_time
                    })
                    
                    # æ£€æŸ¥å“åº”
                    if response.status_code == 200:
                        page_result = response.json()
                        page_success = True
                        self.successful_requests += 1
                        
                        # è®°å½•é™æµä¿¡æ¯
                        remaining = response.headers.get('X-RateLimit-Remaining')
                        if remaining and int(remaining) < 5:
                            logger.warning(f"âš ï¸ Low quota: {remaining} remaining for {mask_key(token)}")
                        
                        break
                    
                    elif response.status_code in (403, 429):
                        # é™æµ
                        rate_limit_hits += 1
                        self.failed_requests += 1
                        logger.warning(f"ğŸš« Rate limited (attempt {attempt}/{max_retries})")
                        
                        # å¿«é€Ÿåˆ‡æ¢ä»¤ç‰Œ
                        if attempt < max_retries:
                            time.sleep(1)  # çŸ­æš‚ç­‰å¾…
                            continue
                    
                    else:
                        # å…¶ä»–é”™è¯¯
                        self.failed_requests += 1
                        logger.error(f"âŒ HTTP {response.status_code} (attempt {attempt}/{max_retries})")
                        
                        if attempt < max_retries:
                            time.sleep(min(2 ** attempt, 10))
                            continue
                
                except requests.exceptions.RequestException as e:
                    failed_requests += 1
                    self.failed_requests += 1
                    
                    if attempt == max_retries:
                        logger.error(f"âŒ Request failed after {max_retries} attempts: {type(e).__name__}")
                    
                    time.sleep(min(2 ** attempt, 10))
                    continue
            
            # å¤„ç†é¡µé¢ç»“æœ
            if not page_success or not page_result:
                if page == 1:
                    logger.error(f"âŒ First page failed for query: {query[:50]}...")
                    # æ”¹è¿›ï¼šç¬¬ä¸€é¡µå¤±è´¥ä¹Ÿå°è¯•é‡è¯•
                    if max_retries > 0:
                        failed_pages.append({'page': page, 'retry_count': 0})
                    else:
                        break
                else:
                    # åŠ å…¥é‡è¯•é˜Ÿåˆ—
                    failed_pages.append({'page': page, 'retry_count': 0})
                    logger.debug(f"ğŸ“ Page {page} added to retry queue")
                continue
            
            pages_processed += 1
            successful_pages.add(page)  # è®°å½•æˆåŠŸé¡µé¢
            
            # å¤„ç†ç¬¬ä¸€é¡µ
            if page == 1:
                total_count = page_result.get("total_count", 0)
                expected_total = min(total_count, 1000)  # GitHub é™åˆ¶æœ€å¤š1000ä¸ªç»“æœ
                logger.info(f"ğŸ” Query: {query} - Total results: {total_count}")
                
                # åŠ¨æ€è°ƒæ•´æœ€å¤§é¡µæ•°
                if expected_total > 0:
                    needed_pages = min((expected_total + 99) // 100, 10)  # å‘ä¸Šå–æ•´
                    max_pages = min(needed_pages, max_pages)
                    logger.debug(f"ğŸ“Š Adjusted max pages to {max_pages} based on total count")
            
            # æ”¶é›†é¡¹ç›®
            items = page_result.get("items", [])
            current_page_count = len(items)
            
            if current_page_count == 0:
                logger.debug(f"ğŸ“„ Page {page} returned 0 items, stopping pagination")
                break
            
            all_items.extend(items)
            logger.debug(f"âœ… Page {page}: collected {current_page_count} items (total: {len(all_items)})")
            
            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if expected_total and len(all_items) >= expected_total:
                logger.info(f"âœ… Collected all expected items ({len(all_items)}/{expected_total})")
                break
            
            # é¡µé¢é—´å»¶è¿Ÿï¼ˆè‡ªé€‚åº”ï¼‰
            if page < max_pages:
                # æ ¹æ®å‰©ä½™é…é¢è°ƒæ•´å»¶è¿Ÿ
                if rate_limit_hits > 0:
                    sleep_time = random.uniform(2.0, 3.0)  # é™æµæ—¶å¢åŠ å»¶è¿Ÿ
                else:
                    sleep_time = random.uniform(0.5, 1.0)  # æ­£å¸¸å»¶è¿Ÿ
                logger.debug(f"â³ Page {page} complete, sleeping {sleep_time:.1f}s")
                time.sleep(sleep_time)
        
        # æ”¹è¿›çš„é‡è¯•å¤±è´¥é¡µé¢é€»è¾‘
        retry_attempts = 0
        max_retry_rounds = 2  # æœ€å¤šè¿›è¡Œ2è½®é‡è¯•
        
        while failed_pages and retry_attempts < max_retry_rounds:
            retry_attempts += 1
            logger.info(f"ğŸ”„ Retry round {retry_attempts}: {len(failed_pages)} failed pages")
            
            # å¤åˆ¶å¤±è´¥é¡µé¢åˆ—è¡¨ç”¨äºè¿­ä»£
            pages_to_retry = failed_pages[:]
            failed_pages = []  # æ¸…ç©ºåŸåˆ—è¡¨
            
            for failed_page_info in pages_to_retry:
                page = failed_page_info['page']
                
                # è·³è¿‡å·²æˆåŠŸçš„é¡µé¢
                if page in successful_pages:
                    continue
                    
                if failed_page_info['retry_count'] >= max_page_retries:
                    logger.warning(f"âš ï¸ Page {page} exceeded max retries")
                    continue
                
                failed_page_info['retry_count'] += 1
                
                # æŒ‡æ•°é€€é¿å»¶è¿Ÿ
                delay = min(2 ** failed_page_info['retry_count'], 10)
                time.sleep(delay)
                
                # é‡è¯•è·å–é¡µé¢
                token = self.token_pool.select_token()
                if not token:
                    logger.warning(f"âš ï¸ No token available for retry of page {page}")
                    failed_pages.append(failed_page_info)
                    continue
                
                try:
                    headers = {
                        "Accept": "application/vnd.github.v3+json",
                        "Authorization": f"token {token}",
                        "User-Agent": "Mozilla/5.0 (compatible; HajimiKing/2.0)"
                    }
                    params = {
                        "q": query,
                        "per_page": 100,
                        "page": page,
                        "sort": "indexed",  # æ·»åŠ æ’åºä»¥æé«˜ä¸€è‡´æ€§
                        "order": "desc"
                    }
                    
                    response = self.session.get(
                        self.GITHUB_API_URL,
                        headers=headers,
                        params=params,
                        timeout=30,
                        proxies=self.proxy_config
                    )
                    
                    # æ›´æ–°ä»¤ç‰ŒçŠ¶æ€
                    self.token_pool.update_token_status(token, {
                        'status_code': response.status_code,
                        'headers': dict(response.headers),
                        'response_time': 0
                    })
                    
                    if response.status_code == 200:
                        retry_result = response.json()
                        items = retry_result.get("items", [])
                        if items:
                            # å»é‡ï¼šé¿å…é‡å¤æ·»åŠ 
                            existing_urls = {item.get('html_url') for item in all_items}
                            new_items = [item for item in items if item.get('html_url') not in existing_urls]
                            
                            if new_items:
                                all_items.extend(new_items)
                                pages_processed += 1
                                successful_pages.add(page)
                                logger.info(f"âœ… Recovered page {page}: {len(new_items)} new items")
                            else:
                                logger.debug(f"ğŸ“ Page {page} recovered but no new items")
                    else:
                        # é‡è¯•å¤±è´¥ï¼Œé‡æ–°åŠ å…¥é˜Ÿåˆ—
                        if failed_page_info['retry_count'] < max_page_retries:
                            failed_pages.append(failed_page_info)
                        logger.debug(f"âŒ Retry failed for page {page}: HTTP {response.status_code}")
                
                except Exception as e:
                    # å¼‚å¸¸æ—¶é‡æ–°åŠ å…¥é˜Ÿåˆ—
                    if failed_page_info['retry_count'] < max_page_retries:
                        failed_pages.append(failed_page_info)
                    logger.debug(f"âŒ Exception retrying page {page}: {type(e).__name__}")
        
        # æ”¹è¿›çš„æ•°æ®å®Œæ•´æ€§è®¡ç®—
        final_count = len(all_items)
        completeness = 100.0
        completeness_status = "COMPLETE"
        
        if expected_total and expected_total > 0:
            completeness = (final_count / expected_total) * 100
            
            if completeness >= 95:
                completeness_status = "EXCELLENT"
            elif completeness >= 80:
                completeness_status = "GOOD"
            elif completeness >= 60:
                completeness_status = "ACCEPTABLE"
            elif completeness >= 40:
                completeness_status = "POOR"
            else:
                completeness_status = "CRITICAL"
            
            # è¯¦ç»†çš„å®Œæ•´æ€§æ—¥å¿—
            if completeness < 95:
                missing_pages = [p for p in range(1, max_pages + 1) if p not in successful_pages]
                if missing_pages:
                    logger.warning(f"âš ï¸ Missing pages: {missing_pages[:10]}{'...' if len(missing_pages) > 10 else ''}")
                
                logger.warning(
                    f"âš ï¸ Data completeness: {completeness:.1f}% ({final_count}/{expected_total}) - {completeness_status}"
                )
            else:
                logger.info(f"âœ… Data completeness: {completeness:.1f}% - {completeness_status}")
        
        # è®°å½•è¯¦ç»†æ‘˜è¦
        logger.info(
            f"ğŸ” Search complete: query={query[:30]}... | "
            f"pages={pages_processed}/{max_pages} | items={final_count}/{expected_total or '?'} | "
            f"completeness={completeness:.1f}% | "
            f"requests={total_requests} | rate_limits={rate_limit_hits}"
        )
        
        return {
            "total_count": total_count,
            "incomplete_results": final_count < expected_total if expected_total else False,
            "items": all_items,
            "completeness_percentage": completeness,
            "completeness_status": completeness_status,
            "statistics": {
                "pages_processed": pages_processed,
                "pages_attempted": max_pages,
                "successful_pages": len(successful_pages),
                "total_requests": total_requests,
                "failed_requests": failed_requests,
                "rate_limit_hits": rate_limit_hits,
                "items_collected": final_count,
                "expected_items": expected_total
            }
        }
    
    def get_file_content(self, item: Dict[str, Any]) -> Optional[str]:
        """
        è·å–æ–‡ä»¶å†…å®¹
        
        Args:
            item: æœç´¢ç»“æœé¡¹
            
        Returns:
            æ–‡ä»¶å†…å®¹æˆ– None
        """
        repo_full_name = item["repository"]["full_name"]
        file_path = item["path"]
        metadata_url = f"https://api.github.com/repos/{repo_full_name}/contents/{file_path}"
        
        # è·å–ä»¤ç‰Œ
        token = self.token_pool.select_token()
        if not token:
            logger.error("âŒ No available token for file content")
            return None
        
        headers = {
            "Accept": "application/vnd.github.v3+json",
            "Authorization": f"token {token}"
        }
        
        try:
            logger.debug(f"ğŸ“„ Fetching: {metadata_url}")
            
            # è·å–æ–‡ä»¶å…ƒæ•°æ®ï¼ˆä½¿ç”¨é…ç½®çš„ä»£ç†ï¼‰
            start_time = time.time()
            metadata_response = self.session.get(
                metadata_url,
                headers=headers,
                timeout=15,
                proxies=self.proxy_config  # æ˜¾å¼ä¼ é€’ä»£ç†é…ç½®
            )
            response_time = time.time() - start_time
            
            # æ›´æ–° TokenPool
            self.token_pool.update_token_status(token, {
                'status_code': metadata_response.status_code,
                'headers': dict(metadata_response.headers),
                'response_time': response_time
            })
            
            if metadata_response.status_code != 200:
                logger.warning(f"âš ï¸ Failed to fetch metadata: HTTP {metadata_response.status_code}")
                return None
            
            file_metadata = metadata_response.json()
            
            # å°è¯• base64 å†…å®¹
            encoding = file_metadata.get("encoding")
            content = file_metadata.get("content")
            
            if encoding == "base64" and content:
                try:
                    decoded_content = base64.b64decode(content).decode('utf-8')
                    return decoded_content
                except Exception as e:
                    logger.debug(f"Base64 decode failed: {e}")
            
            # ä½¿ç”¨ download_url
            download_url = file_metadata.get("download_url")
            if download_url:
                content_response = self.session.get(
                    download_url,
                    headers=headers,
                    timeout=15,
                    proxies=self.proxy_config  # æ˜¾å¼ä¼ é€’ä»£ç†é…ç½®
                )
                if content_response.status_code == 200:
                    return content_response.text
            
            return None
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Failed to fetch file content: {type(e).__name__}")
            return None
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å®¢æˆ·ç«¯ç»Ÿè®¡"""
        return {
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.failed_requests,
            "success_rate": (
                self.successful_requests / self.total_requests 
                if self.total_requests > 0 else 0
            ),
            "token_pool_status": self.token_pool.get_pool_status()
        }
    
    def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        self.session.close()
        logger.info("ğŸ”Œ GitHub Client V2 closed")


# å·¥å‚å‡½æ•°
def create_github_client_v2(tokens: List[str],
                           strategy: str = "ADAPTIVE",
                           proxy_config: Optional[Dict[str, str]] = None) -> GitHubClientV2:
    """
    åˆ›å»º GitHub å®¢æˆ·ç«¯ V2
    
    Args:
        tokens: GitHub ä»¤ç‰Œåˆ—è¡¨
        strategy: TokenPool ç­–ç•¥
        proxy_config: ä»£ç†é…ç½®å­—å…¸ (å¯é€‰)
        
    Returns:
        GitHubClientV2 å®ä¾‹
    """
    # åˆ›å»º TokenPool
    strategy_enum = TokenSelectionStrategy[strategy.upper()]
    token_pool = TokenPool(tokens, strategy=strategy_enum)
    
    # åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä¼ é€’ä»£ç†é…ç½®ï¼‰
    return GitHubClientV2(token_pool, proxy_config)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import os
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)s | %(name)s | %(message)s'
    )
    
    # æµ‹è¯•ä»¤ç‰Œ
    test_tokens = [
        "github_pat_11AAAAAAA" + "A" * 74,
        "github_pat_11BBBBBBBB" + "B" * 74,
    ]
    
    # æµ‹è¯•ä»£ç†é…ç½®
    proxy_config = None
    if os.getenv('HTTP_PROXY'):
        proxy_config = {
            'http': os.getenv('HTTP_PROXY'),
            'https': os.getenv('HTTPS_PROXY', os.getenv('HTTP_PROXY'))
        }
        print(f"ğŸŒ Using proxy configuration: {proxy_config}")
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = create_github_client_v2(test_tokens, strategy="ADAPTIVE", proxy_config=proxy_config)
    
    # æµ‹è¯•æœç´¢
    result = client.search_for_keys("test query", max_retries=3)
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = client.get_statistics()
    print("\nğŸ“Š Client Statistics:")
    for key, value in stats.items():
        if key != "token_pool_status":
            print(f"  {key}: {value}")
    
    # å…³é—­
    client.close()