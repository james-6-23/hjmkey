# V4 æœç´¢å™¨å®ç°ä»£ç 

æœ¬æ–‡æ¡£åŒ…å«æ‰€æœ‰æ–°æœç´¢å™¨çš„å®Œæ•´å®ç°ä»£ç ã€‚

## 1. Web æœç´¢å™¨å®ç°

### æ–‡ä»¶ï¼š`app/features/extended_search/web_searcher.py`

```python
"""
Webå¹³å°Tokenæœç´¢å™¨
æœç´¢Stack Overflowã€Pastebinã€GitHub Gistç­‰å¹³å°çš„æ³„éœ²tokens
"""

import re
import requests
import time
import logging
from typing import List, Set, Dict, Any, Optional
from urllib.parse import quote

logger = logging.getLogger(__name__)


class WebSearcher:
    """Webå¹³å°Tokenæœç´¢å™¨"""
    
    # Tokenæ­£åˆ™æ¨¡å¼
    TOKEN_PATTERNS = [
        re.compile(r'ghp_[a-zA-Z0-9]{36}'),  # GitHub Personal Access Token
        re.compile(r'github_pat_[a-zA-Z0-9]{22}_[a-zA-Z0-9]{59}'),  # GitHub Fine-grained PAT
        re.compile(r'ghs_[a-zA-Z0-9]{36}'),  # GitHub App Token
        re.compile(r'AIzaSy[a-zA-Z0-9_-]{33}'),  # Google API Key
        re.compile(r'sk-[a-zA-Z0-9]{48}'),  # OpenAI API Key
        re.compile(r'glpat-[a-zA-Z0-9]{20}'),  # GitLab Personal Access Token
    ]
    
    def __init__(self, proxy: Optional[Dict[str, str]] = None):
        """
        åˆå§‹åŒ–Webæœç´¢å™¨
        
        Args:
            proxy: ä»£ç†é…ç½®
        """
        self.proxy = proxy
        self.session = requests.Session()
        if proxy:
            self.session.proxies.update(proxy)
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        logger.info("âœ… Webæœç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def search_stackoverflow(self, query: str = "github token api key", max_results: int = 50) -> List[str]:
        """
        æœç´¢Stack Overflow
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ‰¾åˆ°çš„tokenåˆ—è¡¨
        """
        tokens = set()
        logger.info(f"ğŸ” æœç´¢Stack Overflow: {query}")
        
        try:
            # ä½¿ç”¨Stack Exchange API
            api_url = "https://api.stackexchange.com/2.3/search/advanced"
            
            # åˆ†é¡µæœç´¢
            page = 1
            while len(tokens) < max_results and page <= 5:  # æœ€å¤š5é¡µ
                params = {
                    'order': 'desc',
                    'sort': 'creation',
                    'q': query,
                    'site': 'stackoverflow',
                    'pagesize': 100,
                    'page': page,
                    'filter': 'withbody'  # åŒ…å«é—®é¢˜å’Œç­”æ¡ˆçš„å†…å®¹
                }
                
                response = self.session.get(api_url, params=params, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    items = data.get('items', [])
                    
                    if not items:
                        break
                    
                    for item in items:
                        # æœç´¢é—®é¢˜å†…å®¹
                        title = item.get('title', '')
                        body = item.get('body', '')
                        content = f"{title} {body}"
                        
                        # æå–tokens
                        question_tokens = self._extract_tokens_from_text(content)
                        tokens.update(question_tokens)
                        
                        # æœç´¢ç­”æ¡ˆ
                        if 'answers' in item:
                            for answer in item['answers']:
                                answer_body = answer.get('body', '')
                                answer_tokens = self._extract_tokens_from_text(answer_body)
                                tokens.update(answer_tokens)
                    
                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µé¢
                    if not data.get('has_more', False):
                        break
                    
                    page += 1
                    time.sleep(0.5)  # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
                else:
                    logger.warning(f"Stack Overflow APIè¿”å›é”™è¯¯: {response.status_code}")
                    break
        
        except Exception as e:
            logger.error(f"æœç´¢Stack Overflowå¤±è´¥: {e}")
        
        logger.info(f"âœ… Stack Overflowæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(tokens)} ä¸ªtokens")
        return list(tokens)[:max_results]
    
    def search_pastebin_recent(self, max_results: int = 20) -> List[str]:
        """
        æœç´¢Pastebinæœ€è¿‘çš„å…¬å¼€å†…å®¹
        
        Args:
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ‰¾åˆ°çš„tokenåˆ—è¡¨
        """
        tokens = set()
        logger.info("ğŸ” æœç´¢Pastebinæœ€è¿‘å†…å®¹")
        
        try:
            # Pastebin scraping API
            api_url = "https://scrape.pastebin.com/api_scraping.php"
            params = {
                'limit': min(max_results * 5, 250)  # è·å–æ›´å¤šä»¥æé«˜å‘½ä¸­ç‡
            }
            
            response = self.session.get(api_url, params=params, timeout=10)
            if response.status_code == 200:
                pastes = response.json()
                
                for paste in pastes[:max_results * 2]:  # æ£€æŸ¥æ›´å¤špaste
                    paste_key = paste.get('key')
                    if paste_key:
                        # è·å–pasteå†…å®¹
                        content_url = f"https://pastebin.com/raw/{paste_key}"
                        try:
                            content_response = self.session.get(content_url, timeout=5)
                            if content_response.status_code == 200:
                                # æå–tokens
                                paste_tokens = self._extract_tokens_from_text(content_response.text)
                                tokens.update(paste_tokens)
                                
                                if len(tokens) >= max_results:
                                    break
                        except:
                            continue
                        
                        # é¿å…è¢«å°IP
                        time.sleep(1)
            else:
                logger.warning(f"Pastebin APIè¿”å›é”™è¯¯: {response.status_code}")
        
        except Exception as e:
            logger.error(f"æœç´¢Pastebinå¤±è´¥: {e}")
        
        logger.info(f"âœ… Pastebinæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(tokens)} ä¸ªtokens")
        return list(tokens)[:max_results]
    
    def search_github_gist(self, query: str = "AIzaSy", max_results: int = 30) -> List[str]:
        """
        æœç´¢GitHub Gist
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ‰¾åˆ°çš„tokenåˆ—è¡¨
        """
        tokens = set()
        logger.info(f"ğŸ” æœç´¢GitHub Gist: {query}")
        
        try:
            # ä½¿ç”¨GitHubæœç´¢APIæœç´¢Gist
            api_url = "https://api.github.com/search/code"
            
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_query = f'{query} in:file'
            
            params = {
                'q': search_query,
                'per_page': min(max_results, 100),
                'sort': 'indexed',
                'order': 'desc'
            }
            
            headers = {
                'Accept': 'application/vnd.github.v3+json'
            }
            
            response = self.session.get(api_url, params=params, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                items = data.get('items', [])
                
                for item in items:
                    # åªå¤„ç†Gist
                    if 'gist.github.com' in item.get('html_url', ''):
                        # è·å–æ–‡ä»¶å†…å®¹
                        file_url = item.get('url')
                        if file_url:
                            try:
                                file_response = self.session.get(file_url, headers=headers, timeout=5)
                                if file_response.status_code == 200:
                                    file_data = file_response.json()
                                    content = file_data.get('content', '')
                                    
                                    if content:
                                        # Base64è§£ç 
                                        import base64
                                        try:
                                            decoded_content = base64.b64decode(content).decode('utf-8')
                                            gist_tokens = self._extract_tokens_from_text(decoded_content)
                                            tokens.update(gist_tokens)
                                        except:
                                            pass
                            except:
                                continue
                            
                            time.sleep(0.5)  # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
            else:
                logger.warning(f"GitHub APIè¿”å›é”™è¯¯: {response.status_code}")
        
        except Exception as e:
            logger.error(f"æœç´¢GitHub Gistå¤±è´¥: {e}")
        
        logger.info(f"âœ… GitHub Gistæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(tokens)} ä¸ªtokens")
        return list(tokens)[:max_results]
    
    def search_reddit(self, subreddit: str = "programming", max_results: int = 30) -> List[str]:
        """
        æœç´¢Reddit
        
        Args:
            subreddit: å­ç‰ˆå—åç§°
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ‰¾åˆ°çš„tokenåˆ—è¡¨
        """
        tokens = set()
        logger.info(f"ğŸ” æœç´¢Reddit r/{subreddit}")
        
        try:
            # Reddit JSON API
            url = f"https://www.reddit.com/r/{subreddit}/search.json"
            params = {
                'q': 'api key OR token OR "AIzaSy" OR "ghp_"',
                'sort': 'new',
                'limit': min(max_results * 2, 100),
                'restrict_sr': 'true'
            }
            
            headers = {
                'User-Agent': 'TokenHunter/1.0'
            }
            
            response = self.session.get(url, params=params, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                posts = data.get('data', {}).get('children', [])
                
                for post in posts:
                    post_data = post.get('data', {})
                    
                    # æœç´¢æ ‡é¢˜å’Œå†…å®¹
                    title = post_data.get('title', '')
                    selftext = post_data.get('selftext', '')
                    content = f"{title} {selftext}"
                    
                    # æå–tokens
                    post_tokens = self._extract_tokens_from_text(content)
                    tokens.update(post_tokens)
                    
                    # ä¹Ÿæœç´¢è¯„è®ºï¼ˆå¦‚æœæœ‰URLï¼‰
                    if post_data.get('num_comments', 0) > 0:
                        comments_url = f"https://www.reddit.com{post_data.get('permalink', '')}.json"
                        try:
                            comments_response = self.session.get(comments_url, headers=headers, timeout=5)
                            if comments_response.status_code == 200:
                                comments_data = comments_response.json()
                                # ç®€å•å¤„ç†ç¬¬ä¸€å±‚è¯„è®º
                                if len(comments_data) > 1:
                                    comments = comments_data[1].get('data', {}).get('children', [])
                                    for comment in comments[:10]:  # åªçœ‹å‰10æ¡è¯„è®º
                                        comment_body = comment.get('data', {}).get('body', '')
                                        comment_tokens = self._extract_tokens_from_text(comment_body)
                                        tokens.update(comment_tokens)
                        except:
                            pass
                    
                    time.sleep(0.5)  # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
            else:
                logger.warning(f"Reddit APIè¿”å›é”™è¯¯: {response.status_code}")
        
        except Exception as e:
            logger.error(f"æœç´¢Redditå¤±è´¥: {e}")
        
        logger.info(f"âœ… Redditæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(tokens)} ä¸ªtokens")
        return list(tokens)[:max_results]
    
    def _extract_tokens_from_text(self, text: str) -> Set[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–tokens
        
        Args:
            text: è¦æœç´¢çš„æ–‡æœ¬
            
        Returns:
            æ‰¾åˆ°çš„tokené›†åˆ
        """
        tokens = set()
        if not text:
            return tokens
        
        # ä½¿ç”¨æ‰€æœ‰tokenæ¨¡å¼è¿›è¡ŒåŒ¹é…
        for pattern in self.TOKEN_PATTERNS:
            matches = pattern.findall(text)
            tokens.update(matches)
        
        # è¿‡æ»¤æ˜æ˜¾çš„å ä½ç¬¦
        filtered_tokens = set()
        for token in tokens:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å ä½ç¬¦
            if not any(placeholder in token.lower() for placeholder in ['xxx', 'your', 'example', 'test', 'demo']):
                # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯é‡å¤å­—ç¬¦
                if len(set(token[10:])) > 5:  # å‰ç¼€åçš„éƒ¨åˆ†æœ‰è¶³å¤Ÿçš„å­—ç¬¦å¤šæ ·æ€§
                    filtered_tokens.add(token)
        
        return filtered_tokens
    
    def search_all_platforms(self, max_results_per_platform: int = 20) -> Dict[str, List[str]]:
        """
        æœç´¢æ‰€æœ‰å¹³å°
        
        Args:
            max_results_per_platform: æ¯ä¸ªå¹³å°çš„æœ€å¤§ç»“æœæ•°
            
        Returns:
            å¹³å°åˆ°tokenåˆ—è¡¨çš„æ˜ å°„
        """
        results = {}
        
        logger.info("ğŸ” å¼€å§‹æœç´¢æ‰€æœ‰Webå¹³å°...")
        
        # Stack Overflow
        logger.info("æœç´¢Stack Overflow...")
        results['stackoverflow'] = self.search_stackoverflow(max_results=max_results_per_platform)
        
        # Pastebin
        logger.info("æœç´¢Pastebin...")
        results['pastebin'] = self.search_pastebin_recent(max_results=max_results_per_platform)
        
        # GitHub Gist
        logger.info("æœç´¢GitHub Gist...")
        results['github_gist'] = self.search_github_gist(max_results=max_results_per_platform)
        
        # Reddit
        logger.info("æœç´¢Reddit...")
        results['reddit'] = self.search_reddit(max_results=max_results_per_platform)
        
        # ç»Ÿè®¡ç»“æœ
        total_tokens = sum(len(tokens) for tokens in results.values())
        logger.info(f"âœ… Webæœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {total_tokens} ä¸ªæ½œåœ¨tokens")
        
        for platform, tokens in results.items():
            logger.info(f"   {platform}: {len(tokens)} tokens")
        
        return results
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'session'):
            self.session.close()
```

## 2. GitLab æœç´¢å™¨å®ç°

### æ–‡ä»¶ï¼š`app/features/extended_search/gitlab_searcher.py`

```python
"""
GitLab Tokenæœç´¢å™¨
æœç´¢GitLabå…¬å¼€ä»“åº“ä¸­çš„æ³„éœ²tokens
"""

import re
import requests
import time
import logging
from typing import List, Set, Dict, Any, Optional
from urllib.parse import quote

logger = logging.getLogger(__name__)


class GitLabSearcher:
    """GitLab Tokenæœç´¢å™¨"""
    
    # GitLabå®ä¾‹åˆ—è¡¨
    GITLAB_INSTANCES = [
        "https://gitlab.com",  # å®˜æ–¹GitLab
        # å¯ä»¥æ·»åŠ å…¶ä»–å…¬å¼€çš„GitLabå®ä¾‹
    ]
    
    # Tokenæ­£åˆ™æ¨¡å¼
    TOKEN_PATTERNS = [
        re.compile(r'ghp_[a-zA-Z0-9]{36}'),  # GitHub token
        re.compile(r'github_pat_[a-zA-Z0-9]{22}_[a-zA-Z0-9]{59}'),
        re.compile(r'ghs_[a-zA-Z0-9]{36}'),
        re.compile(r'AIzaSy[a-zA-Z0-9_-]{33}'),  # Google API Key
        re.compile(r'glpat-[a-zA-Z0-9]{20}'),  # GitLab Personal Access Token
        re.compile(r'gpldt-[a-zA-Z0-9]{20}'),  # GitLab Deploy Token
        re.compile(r'gldt-[a-zA-Z0-9]{20}'),  # GitLab Deploy Token (alternative)
    ]
    
    def __init__(self, access_token: Optional[str] = None, proxy: Optional[Dict] = None, gitlab_url: str = None):
        """
        åˆå§‹åŒ–GitLabæœç´¢å™¨
        
        Args:
            access_token: GitLabè®¿é—®ä»¤ç‰Œï¼ˆå¯é€‰ï¼Œç”¨äºæé«˜APIé™åˆ¶ï¼‰
            proxy: ä»£ç†é…ç½®
            gitlab_url: GitLabå®ä¾‹URLï¼ˆé»˜è®¤ä½¿ç”¨gitlab.comï¼‰
        """
        self.access_token = access_token
        self.proxy = proxy
        self.gitlab_url = gitlab_url or self.GITLAB_INSTANCES[0]
        self.api_base = f"{self.gitlab_url}/api/v4"
        
        self.session = requests.Session()
        if proxy:
            self.session.proxies.update(proxy)
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'TokenHunter/1.0'
        }
        
        if access_token:
            self.headers['Authorization'] = f'Bearer {access_token}'
        
        logger.info(f"âœ… GitLabæœç´¢å™¨åˆå§‹åŒ–å®Œæˆ - {self.gitlab_url}")
    
    def search(self, query: str = "AIzaSy", max_results: int = 100) -> List[str]:
        """
        æœç´¢GitLabå…¬å¼€é¡¹ç›®
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ‰¾åˆ°çš„tokenåˆ—è¡¨
        """
        tokens = set()
        logger.info(f"ğŸ” æœç´¢GitLab: {query}")
        
        # æœç´¢ä¸åŒçš„èŒƒå›´
        scopes = ['blobs', 'issues', 'merge_requests', 'commits']
        
        for scope in scopes:
            if len(tokens) >= max_results:
                break
            
            logger.info(f"æœç´¢èŒƒå›´: {scope}")
            scope_tokens = self._search_scope(query, scope, max_results - len(tokens))
            tokens.update(scope_tokens)
        
        logger.info(f"âœ… GitLabæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(tokens)} ä¸ªtokens")
        return list(tokens)[:max_results]
    
    def _search_scope(self, query: str, scope: str, max_results: int) -> Set[str]:
        """
        åœ¨ç‰¹å®šèŒƒå›´å†…æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            scope: æœç´¢èŒƒå›´
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ‰¾åˆ°çš„tokené›†åˆ
        """
        tokens = set()
        
        try:
            # GitLabæœç´¢API
            search_url = f"{self.api_base}/search"
            
            page = 1
            per_page = 20
            
            while len(tokens) < max_results and page <= 5:  # æœ€å¤š5é¡µ
                params = {
                    'scope': scope,
                    'search': query,
                    'per_page': per_page,
                    'page': page
                }
                
                response = self.session.get(
                    search_url,
                    params=params,
                    headers=self.headers,
                    timeout=10
                )
                
                if response.status_code == 200:
                    results = response.json()
                    
                    if not results:
                        break
                    
                    # æ ¹æ®ä¸åŒçš„scopeå¤„ç†ç»“æœ
                    if scope == 'blobs':
                        for item in results:
                            # è·å–æ–‡ä»¶å†…å®¹
                            file_tokens = self._extract_from_blob(item)
                            tokens.update(file_tokens)
                    
                    elif scope == 'issues':
                        for item in results:
                            # ä»issueä¸­æå–
                            title = item.get('title', '')
                            description = item.get('description', '')
                            content = f"{title} {description}"
                            issue_tokens = self._extract_tokens_from_text(content)
                            tokens.update(issue_tokens)
                    
                    elif scope == 'merge_requests':
                        for item in results:
                            # ä»MRä¸­æå–
                            title = item.get('title', '')
                            description = item.get('description', '')
                            content = f"{title} {description}"
                            mr_tokens = self._extract_tokens_from_text(content)
                            tokens.update(mr_tokens)
                    
                    elif scope == 'commits':
                        for item in results:
                            # ä»commitæ¶ˆæ¯ä¸­æå–
                            message = item.get('message', '')
                            title = item.get('title', '')
                            content = f"{title} {message}"
                            commit_tokens = self._extract_tokens_from_text(content)
                            tokens.update(commit_tokens)
                    
                    page += 1
                    time.sleep(0.5)  # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
                
                elif response.status_code == 401:
                    logger.warning("GitLab APIè®¤è¯å¤±è´¥")
                    break
                elif response.status_code == 429:
                    logger.warning("GitLab APIé€Ÿç‡é™åˆ¶")
                    break
                else:
                    logger.warning(f"GitLab APIè¿”å›é”™è¯¯: {response.status_code}")
                    break
        
        except Exception as e:
            logger.error(f"æœç´¢GitLab {scope} å¤±è´¥: {e}")
        
        return tokens
    
    def _extract_from_blob(self, blob_item: Dict) -> Set[str]:
        """
        ä»blobï¼ˆæ–‡ä»¶ï¼‰ç»“æœä¸­æå–tokens
        
        Args:
            blob_item: blobæœç´¢ç»“æœé¡¹
            
        Returns:
            æ‰¾åˆ°çš„tokené›†åˆ
        """
        tokens = set()
        
        try:
            # è·å–é¡¹ç›®IDå’Œæ–‡ä»¶è·¯å¾„
            project_id = blob_item.get('project_id')
            file_path = blob_item.get('path')
            ref = blob_item.get('ref', 'master')
            
            if project_id and file_path:
                # æ„å»ºæ–‡ä»¶å†…å®¹URL
                file_url = f"{self.api_base}/projects/{project_id}/repository/files/{quote(file_path, safe='')}/raw"
                params = {'ref': ref}
                
                response = self.session.get(
                    file_url,
                    params=params,
                    headers=self.headers,
                    timeout=5
                )
                
                if response.status_code == 200:
                    content = response.text
                    # é™åˆ¶å†…å®¹å¤§å°ï¼Œé¿å…å¤„ç†è¿‡å¤§çš„æ–‡ä»¶
                    if len(content) < 1024 * 1024:  # 1MB
                        file_tokens = self._extract_tokens_from_text(content)
                        tokens.update(file_tokens)
                
        except Exception as e:
            logger.debug(f"æå–GitLabæ–‡ä»¶å¤±è´¥: {e}")
        
        return tokens
    
    def search_user_projects(self, username: str, max_results: int = 50) -> List[str]:
        """
        æœç´¢ç‰¹å®šç”¨æˆ·çš„å…¬å¼€é¡¹ç›®
        
        Args:
            username: GitLabç”¨æˆ·å
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ‰¾åˆ°çš„tokenåˆ—è¡¨
        """
        tokens = set()
        logger.info(f"ğŸ” æœç´¢GitLabç”¨æˆ· {username} çš„é¡¹ç›®")
        
        try:
            # è·å–ç”¨æˆ·ä¿¡æ¯
            user_url = f"{self.api_base}/users"
            params = {'username': username}
            
            response = self.session.get(user_url, params=params, headers=self.headers, timeout=10)
            if response.status_code == 200:
                users = response.json()
                if users:
                    user_id = users[0].get('id')
                    
                    # è·å–ç”¨æˆ·çš„å…¬å¼€é¡¹ç›®
                    projects_url = f"{self.api_base}/users/{user_id}/projects"
                    params = {'per_page': 100}
                    
                    response = self.session.get(projects_url, params=params, headers=self.headers, timeout=10)
                    if response.status_code == 200:
                        projects = response.json()
                        
                        for project in projects[:20]:  # æœ€å¤šæ£€æŸ¥20ä¸ªé¡¹ç›®
                            project_id = project.get('id')
                            project_name = project.get('name')
                            
                            logger.info(f"æ£€æŸ¥é¡¹ç›®: {project_name}")
                            
                            # æœç´¢é¡¹ç›®ä¸­çš„æ–‡ä»¶
                            project_tokens = self._search_project_files(project_id, max_results=10)
                            tokens.update(project_tokens)
                            
                            if len(tokens) >= max_results:
                                break
        
        except Exception as e:
            logger.error(f"æœç´¢GitLabç”¨æˆ·é¡¹ç›®å¤±è´¥: {e}")
        
        logger.info(f"âœ… ç”¨æˆ·é¡¹ç›®æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(tokens)} ä¸ªtokens")
        return list(tokens)[:max_results]
    
    def _search_project_files(self, project_id: int, max_results: int = 20) -> Set[str]:
        """
        æœç´¢é¡¹ç›®ä¸­çš„æ–‡ä»¶
        
        Args:
            project_id: é¡¹ç›®ID
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ‰¾åˆ°çš„tokené›†åˆ
        """
        tokens = set()
        
        try:
            # è·å–é¡¹ç›®çš„æ–‡ä»¶æ ‘
            tree_url = f"{self.api_base}/projects/{project_id}/repository/tree"
            params = {
                'recursive': 'true',
                'per_page': 100
            }
            
            response = self.session.get(tree_url, params=params, headers=self.headers, timeout=10)
            if response.status_code == 200:
                files = response.json()
                
                # ä¼˜å…ˆæ£€æŸ¥çš„æ–‡ä»¶
                priority_files = ['.env', 'config.json', 'config.yml', 'settings.py', 'application.properties']
                
                # å…ˆæ£€æŸ¥ä¼˜å…ˆæ–‡ä»¶
                for file_item in files:
                    if file_item.get('type') == 'blob':
                        file_name = file_item.get('name', '').lower()
                        file_path = file_item.get('path', '')
                        
                        if any(pf in file_name for pf in priority_files):
                            # è·å–æ–‡ä»¶å†…å®¹
                            file_url = f"{self.api_base}/projects/{project_id}/repository/files/{quote(file_path, safe='')}/raw"
                            
                            try:
                                file_response = self.session.get(file_url, headers=self.headers, timeout=5)
                                if file_response.status_code == 200:
                                    content = file_response.text
                                    if len(content) < 100 * 1024:  # 100KB
                                        file_tokens = self._extract_tokens_from_text(content)
                                        tokens.update(file_tokens)
                            except:
                                pass
                            
                            if len(tokens) >= max_results:
                                break
                            
                            time.sleep(0.2)  # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
        
        except Exception as e:
            logger.debug(f"æœç´¢é¡¹ç›®æ–‡ä»¶å¤±è´¥: {e}")
        
        return tokens
    
    def _extract_tokens_from_text(self, text: str) -> Set[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–tokens
        
        Args:
            text: è¦æœç´¢çš„æ–‡æœ¬
            
        Returns:
            æ‰¾åˆ°çš„tokené›†åˆ
        """
        tokens = set()
        if not text:
            return tokens
        
        # ä½¿ç”¨æ‰€æœ‰tokenæ¨¡å¼è¿›è¡ŒåŒ¹é…
        for pattern in self.TOKEN_PATTERNS:
            matches = pattern.findall(text)
            tokens.update(matches)
        
        # è¿‡æ»¤æ˜æ˜¾çš„å ä½ç¬¦
        filtered_tokens = set()
        for token in tokens:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å ä½ç¬¦
            if not any(placeholder in token.lower() for placeholder in ['xxx', 'your', 'example', 'test', 'demo', 'sample']):
                # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯é‡å¤å­—ç¬¦
                if len(set(token[10:])) > 5:  # å‰ç¼€åçš„éƒ¨åˆ†æœ‰è¶³å¤Ÿçš„å­—ç¬¦å¤šæ ·æ€§
                    filtered_tokens.add(token)
        
        return filtered_tokens
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'session'):
            self.session.close()
```

## 3. Docker æœç´¢å™¨å®ç°

### æ–‡ä»¶ï¼š`app/features/extended_search/docker_searcher.py`

```python
"""
Dockeré•œåƒTokenæœç´¢å™¨
æœç´¢Docker Hubå…¬å¼€é•œåƒä¸­çš„æ³„éœ²tokens
"""

import re
import json
import tarfile
import tempfile
import logging
from pathlib import Path
from typing import List, Set, Dict, Any, Optional
import docker
import requests

logger = logging.getLogger(__name__)


class DockerSearcher:
    """Dockeré•œåƒTokenæœç´¢å™¨"""
    
    # Tokenæ­£åˆ™æ¨¡å¼
    TOKEN_PATTERNS = [
        re.compile(r'ghp_[a-zA-Z0-9]{36}'),
        re.compile(r'github_pat_[a