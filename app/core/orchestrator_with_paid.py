"""
åè°ƒå™¨æ¨¡å— - å¢å¼ºç‰ˆï¼ˆåŒ…å«ä»˜è´¹å¯†é’¥è¯†åˆ«ï¼‰
è´Ÿè´£åè°ƒæ•´ä¸ªæ‰«æã€éªŒè¯å’ŒåŒæ­¥æµç¨‹
"""

import asyncio
import time
import logging
import sys
from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass, field
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from app.core.scanner import Scanner, ScanResult, ScanFilter
from app.core.validator import KeyValidator, ValidationResult, ValidationStatus
from app.core.container import inject
from app.services.config_service import get_config_service
from utils.github_client import GitHubClient

logger = logging.getLogger(__name__)


@dataclass
class OrchestrationConfig:
    """åè°ƒå™¨é…ç½®"""
    max_concurrent_searches: int = 5
    max_concurrent_validations: int = 10
    batch_size: int = 20
    checkpoint_interval: int = 20
    loop_delay: int = 10
    enable_async: bool = True


@dataclass
class OrchestrationStats:
    """åè°ƒç»Ÿè®¡ä¿¡æ¯"""
    total_queries_processed: int = 0
    total_items_processed: int = 0
    total_keys_found: int = 0
    total_valid_keys: int = 0
    total_rate_limited_keys: int = 0
    total_errors: int = 0
    start_time: datetime = field(default_factory=datetime.now)
    
    @property
    def elapsed_time(self) -> float:
        """è·å–è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰"""
        return (datetime.now() - self.start_time).total_seconds()
    
    @property
    def processing_rate(self) -> float:
        """è·å–å¤„ç†é€Ÿç‡ï¼ˆé¡¹/ç§’ï¼‰"""
        if self.elapsed_time > 0:
            return self.total_items_processed / self.elapsed_time
        return 0
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "total_queries_processed": self.total_queries_processed,
            "total_items_processed": self.total_items_processed,
            "total_keys_found": self.total_keys_found,
            "total_valid_keys": self.total_valid_keys,
            "total_rate_limited_keys": self.total_rate_limited_keys,
            "total_errors": self.total_errors,
            "elapsed_time": self.elapsed_time,
            "processing_rate": self.processing_rate,
            "start_time": self.start_time.isoformat()
        }


class Orchestrator:
    """
    ä¸»åè°ƒå™¨
    è´Ÿè´£åè°ƒæ‰«æã€éªŒè¯å’ŒåŒæ­¥çš„æ•´ä½“æµç¨‹
    """
    
    def __init__(
        self,
        scanner: Scanner,
        validator: KeyValidator,
        config: Optional[OrchestrationConfig] = None
    ):
        """
        åˆå§‹åŒ–åè°ƒå™¨
        
        Args:
            scanner: æ‰«æå™¨å®ä¾‹
            validator: éªŒè¯å™¨å®ä¾‹
            config: åè°ƒå™¨é…ç½®
        """
        self.scanner = scanner
        self.validator = validator
        self.config = config or OrchestrationConfig()
        self.stats = OrchestrationStats()
        self.running = False
        self._executor = ThreadPoolExecutor(max_workers=self.config.max_concurrent_searches)
        
        # åˆå§‹åŒ–GitHubå®¢æˆ·ç«¯
        config_service = get_config_service()
        tokens = config_service.get("GITHUB_TOKENS_LIST", [])
        if tokens:
            self.github_client = GitHubClient(tokens)
            logger.info(f"âœ… GitHubå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ {len(tokens)} ä¸ªtokens")
        else:
            self.github_client = None
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„GitHub tokensï¼Œæœç´¢åŠŸèƒ½å°†æ— æ³•ä½¿ç”¨")
        
        # å­˜å‚¨æ‰¾åˆ°çš„æœ‰æ•ˆå¯†é’¥
        self.valid_keys_found = set()
        self.rate_limited_keys_found = set()
        self.paid_keys_found = set()  # ä»˜è´¹ç‰ˆå¯†é’¥
        
    async def run(self, queries: List[str], max_loops: Optional[int] = None) -> OrchestrationStats:
        """
        è¿è¡Œä¸»åè°ƒæµç¨‹
        
        Args:
            queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
            max_loops: æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼ˆNoneè¡¨ç¤ºæ— é™å¾ªç¯ï¼‰
            
        Returns:
            åè°ƒç»Ÿè®¡ä¿¡æ¯
        """
        self.running = True
        loop_count = 0
        
        logger.info("=" * 60)
        logger.info("ğŸš€ ORCHESTRATOR STARTING")
        logger.info("=" * 60)
        logger.info(f"â° Started at: {self.stats.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"ğŸ” Queries to process: {len(queries)}")
        logger.info(f"âš™ï¸ Async mode: {'Enabled' if self.config.enable_async else 'Disabled'}")
        logger.info("=" * 60)
        
        try:
            while self.running and (max_loops is None or loop_count < max_loops):
                loop_count += 1
                logger.info(f"ğŸ”„ Loop #{loop_count} - {datetime.now().strftime('%H:%M:%S')}")
                
                # é‡ç½®æ‰«æå™¨ç»Ÿè®¡
                self.scanner.reset_skip_stats()
                
                # å¤„ç†æŸ¥è¯¢
                if self.config.enable_async:
                    await self._process_queries_async(queries)
                else:
                    self._process_queries_sync(queries)
                
                # æ˜¾ç¤ºå¾ªç¯ç»Ÿè®¡
                self._log_loop_stats(loop_count)
                
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æŸ¥è¯¢éƒ½å·²å¤„ç†
                if self._all_queries_processed(queries):
                    logger.info("âœ… All queries processed, stopping orchestrator")
                    break
                
                # å¾ªç¯é—´å»¶è¿Ÿ
                if self.running and (max_loops is None or loop_count < max_loops):
                    logger.info(f"ğŸ’¤ Sleeping for {self.config.loop_delay} seconds...")
                    await asyncio.sleep(self.config.loop_delay)
                    
        except KeyboardInterrupt:
            logger.info("â›” Interrupted by user")
        except Exception as e:
            logger.error(f"ğŸ’¥ Orchestrator error: {e}")
            self.stats.total_errors += 1
        finally:
            self.running = False
            self._log_final_stats()
            
        return self.stats
    
    async def _process_queries_async(self, queries: List[str]) -> None:
        """
        å¼‚æ­¥å¤„ç†æŸ¥è¯¢åˆ—è¡¨
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
        """
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = []
        semaphore = asyncio.Semaphore(self.config.max_concurrent_searches)
        
        for query in queries:
            if self.scanner.should_skip_query(query):
                logger.info(f"â­ï¸ Skipping already processed query: {query}")
                continue
                
            task = self._process_single_query_async(query, semaphore)
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"Query processing error: {result}")
                    self.stats.total_errors += 1
    
    async def _process_single_query_async(self, query: str, semaphore: asyncio.Semaphore) -> None:
        """
        å¼‚æ­¥å¤„ç†å•ä¸ªæŸ¥è¯¢
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        """
        try:
            async with semaphore:
                # è¿™é‡Œåº”è¯¥è°ƒç”¨å¼‚æ­¥çš„GitHubå®¢æˆ·ç«¯
                # ç”±äºåŸä»£ç ä¸­çš„GitHubå®¢æˆ·ç«¯æ˜¯åŒæ­¥çš„ï¼Œæˆ‘ä»¬åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œ
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(
                    self._executor,
                    self._process_single_query_sync,
                    query
                )
        except asyncio.CancelledError:
            # ä»»åŠ¡è¢«å–æ¶ˆï¼ˆä¾‹å¦‚ç”¨æˆ·æŒ‰Ctrl+Cï¼‰
            logger.info(f"â›” Query cancelled: {query}")
            raise
        except Exception as e:
            logger.error(f"âŒ Error processing query '{query}': {e}")
            self.stats.total_errors += 1
    
    def _process_queries_sync(self, queries: List[str]) -> None:
        """
        åŒæ­¥å¤„ç†æŸ¥è¯¢åˆ—è¡¨
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
        """
        for query in queries:
            if not self.running:
                break
                
            if self.scanner.should_skip_query(query):
                logger.info(f"â­ï¸ Skipping already processed query: {query}")
                continue
                
            self._process_single_query_sync(query)
    
    def _process_single_query_sync(self, query: str) -> None:
        """
        åŒæ­¥å¤„ç†å•ä¸ªæŸ¥è¯¢
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        try:
            logger.info(f"ğŸ” Processing query: {query}")
            
            # æ ‡å‡†åŒ–æŸ¥è¯¢
            normalized_query = self.scanner.normalize_query(query)
            
            # è°ƒç”¨GitHubå®¢æˆ·ç«¯è¿›è¡Œæœç´¢
            if not self.github_client:
                logger.error("âŒ GitHubå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œæœç´¢")
                return
            
            search_result = self.github_client.search_for_keys(query)
            
            if not search_result or not search_result.get("items"):
                logger.info(f"ğŸ“­ No items found for query: {query}")
                self.scanner.filter.add_processed_query(normalized_query)
                self.stats.total_queries_processed += 1
                return
            
            items = search_result["items"]
            query_stats = {
                "valid_keys": 0,
                "rate_limited_keys": 0,
                "processed_items": 0,
                "paid_keys": 0
            }
            
            # å¤„ç†æœç´¢ç»“æœé¡¹
            for i, item in enumerate(items, 1):
                if not self.running:
                    break
                    
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if i % self.config.checkpoint_interval == 0:
                    self._save_checkpoint()
                    logger.info(f"ğŸ“ˆ Progress: {i}/{len(items)} items processed")
                
                # å¤„ç†å•ä¸ªé¡¹
                result = self._process_item(item)
                
                # æ›´æ–°ç»Ÿè®¡
                query_stats["processed_items"] += result.processed_items
                query_stats["valid_keys"] += len(result.valid_keys)
                query_stats["rate_limited_keys"] += len(result.rate_limited_keys)
                
                self.stats.total_items_processed += result.processed_items
                self.stats.total_keys_found += len(result.valid_keys) + len(result.rate_limited_keys)
                self.stats.total_valid_keys += len(result.valid_keys)
                self.stats.total_rate_limited_keys += len(result.rate_limited_keys)
            
            # è®°å½•æŸ¥è¯¢å®Œæˆ
            self.scanner.filter.add_processed_query(normalized_query)
            self.stats.total_queries_processed += 1
            
            # è®°å½•æŸ¥è¯¢ç»Ÿè®¡
            logger.info(
                f"âœ… Query complete - Processed: {query_stats['processed_items']}, "
                f"Valid: {query_stats['valid_keys']}, "
                f"Paid: {len(self.paid_keys_found)}, "
                f"Rate limited: {query_stats['rate_limited_keys']}"
            )
            
            # æ˜¾ç¤ºè·³è¿‡ç»Ÿè®¡
            skip_summary = self.scanner.get_skip_stats_summary()
            if skip_summary != "No items skipped":
                logger.info(f"ğŸ“Š {skip_summary}")
                
        except Exception as e:
            logger.error(f"Error processing query '{query}': {e}")
            self.stats.total_errors += 1
    
    def _process_item(self, item: Dict[str, Any]) -> ScanResult:
        """
        å¤„ç†å•ä¸ªæœç´¢ç»“æœé¡¹
        
        Args:
            item: æœç´¢ç»“æœé¡¹
            
        Returns:
            æ‰«æç»“æœ
        """
        # ä½¿ç”¨æ‰«æå™¨å¤„ç†é¡¹
        result = self.scanner.process_search_item(item)
        
        if result.skipped_items > 0:
            return result
        
        # è·å–æ–‡ä»¶å†…å®¹
        if not self.github_client:
            logger.error("âŒ GitHubå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•è·å–æ–‡ä»¶å†…å®¹")
            return result
        
        content = self.github_client.get_file_content(item)
        
        if not content:
            logger.warning(f"âš ï¸ Failed to fetch content for: {item.get('path', 'unknown')}")
            return result
        
        # æå–å¯†é’¥
        keys = self.scanner.extract_keys_from_content(content)
        
        if not keys:
            return result
        
        logger.info(f"ğŸ”‘ Found {len(keys)} suspected key(s), validating...")
        
        # éªŒè¯å¯†é’¥
        validation_results = self.validator.validate_batch(keys)
        
        for val_result in validation_results:
            if val_result.is_valid:
                result.add_valid_key(val_result.key)
                logger.info(f"âœ… VALID: {val_result.key[:10]}...")
                # ä¿å­˜æœ‰æ•ˆå¯†é’¥
                self.valid_keys_found.add(val_result.key)
                self._save_valid_key(val_result.key)
                # æ£€æŸ¥æ˜¯å¦ä¸ºä»˜è´¹ç‰ˆå¯†é’¥
                if self._check_if_paid_key(val_result.key):
                    self.paid_keys_found.add(val_result.key)
                    self._save_paid_key(val_result.key)
                    logger.info(f"ğŸ’ PAID VERSION: {val_result.key[:10]}...")
            elif val_result.is_rate_limited:
                result.add_rate_limited_key(val_result.key)
                logger.warning(f"âš ï¸ RATE LIMITED: {val_result.key[:10]}...")
                # ä¿å­˜è¢«é™æµçš„å¯†é’¥ï¼ˆå¯èƒ½æ˜¯æœ‰æ•ˆçš„ï¼‰
                self.rate_limited_keys_found.add(val_result.key)
                self._save_rate_limited_key(val_result.key)
            else:
                logger.info(f"âŒ INVALID: {val_result.key[:10]}... - {val_result.status.value}")
        
        return result
    
    def _save_valid_key(self, key: str) -> None:
        """
        ä¿å­˜æœ‰æ•ˆçš„Geminiå¯†é’¥
        
        Args:
            key: æœ‰æ•ˆçš„å¯†é’¥
        """
        try:
            # ç¡®ä¿data/keysç›®å½•å­˜åœ¨
            keys_dir = Path("data/keys")
            keys_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜åˆ°æœ‰æ•ˆå¯†é’¥æ–‡ä»¶
            valid_keys_file = keys_dir / f"keys_valid_{datetime.now().strftime('%Y%m%d')}.txt"
            
            # è¿½åŠ æ¨¡å¼å†™å…¥ï¼Œé¿å…é‡å¤
            existing_keys = set()
            if valid_keys_file.exists():
                with open(valid_keys_file, 'r', encoding='utf-8') as f:
                    existing_keys = set(line.strip() for line in f if line.strip())
            
            if key not in existing_keys:
                with open(valid_keys_file, 'a', encoding='utf-8') as f:
                    f.write(f"{key}\n")
                logger.info(f"ğŸ’¾ ä¿å­˜æœ‰æ•ˆå¯†é’¥åˆ°: {valid_keys_file}")
                
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æœ‰æ•ˆå¯†é’¥å¤±è´¥: {e}")
    
    def _save_rate_limited_key(self, key: str) -> None:
        """
        ä¿å­˜è¢«é™æµçš„å¯†é’¥ï¼ˆå¯èƒ½æ˜¯æœ‰æ•ˆçš„ï¼‰
        
        Args:
            key: è¢«é™æµçš„å¯†é’¥
        """
        try:
            # ç¡®ä¿data/keysç›®å½•å­˜åœ¨
            keys_dir = Path("data/keys")
            keys_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜åˆ°é™æµå¯†é’¥æ–‡ä»¶
            rate_limited_file = keys_dir / f"key_429_{datetime.now().strftime('%Y%m%d')}.txt"
            
            # è¿½åŠ æ¨¡å¼å†™å…¥ï¼Œé¿å…é‡å¤
            existing_keys = set()
            if rate_limited_file.exists():
                with open(rate_limited_file, 'r', encoding='utf-8') as f:
                    existing_keys = set(line.strip() for line in f if line.strip())
            
            if key not in existing_keys:
                with open(rate_limited_file, 'a', encoding='utf-8') as f:
                    f.write(f"{key}\n")
                logger.debug(f"ğŸ’¾ ä¿å­˜é™æµå¯†é’¥åˆ°: {rate_limited_file}")
                
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜é™æµå¯†é’¥å¤±è´¥: {e}")
    
    def _save_paid_key(self, key: str) -> None:
        """
        ä¿å­˜ä»˜è´¹ç‰ˆå¯†é’¥
        
        Args:
            key: ä»˜è´¹ç‰ˆå¯†é’¥
        """
        try:
            # ç¡®ä¿data/keysç›®å½•å­˜åœ¨
            keys_dir = Path("data/keys")
            keys_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜åˆ°ä»˜è´¹å¯†é’¥æ–‡ä»¶
            paid_keys_file = keys_dir / f"keys_paid_{datetime.now().strftime('%Y%m%d')}.txt"
            
            # è¿½åŠ æ¨¡å¼å†™å…¥ï¼Œé¿å…é‡å¤
            existing_keys = set()
            if paid_keys_file.exists():
                with open(paid_keys_file, 'r', encoding='utf-8') as f:
                    existing_keys = set(line.strip() for line in f if line.strip())
            
            if key not in existing_keys:
                with open(paid_keys_file, 'a', encoding='utf-8') as f:
                    f.write(f"{key}\n")
                logger.info(f"ğŸ’ ä¿å­˜ä»˜è´¹ç‰ˆå¯†é’¥åˆ°: {paid_keys_file}")
                
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä»˜è´¹ç‰ˆå¯†é’¥å¤±è´¥: {e}")
    
    def _check_if_paid_key(self, key: str) -> bool:
        """
        æ£€æŸ¥å¯†é’¥æ˜¯å¦ä¸ºä»˜è´¹ç‰ˆæœ¬
        é€šè¿‡å°è¯•è®¿é—®é«˜çº§æ¨¡å‹æ¥åˆ¤æ–­
        
        Args:
            key: è¦æ£€æŸ¥çš„å¯†é’¥
            
        Returns:
            æ˜¯å¦ä¸ºä»˜è´¹ç‰ˆæœ¬
        """
        try:
            import google.generativeai as genai
            
            # é…ç½®APIå¯†é’¥
            genai.configure(api_key=key)
            
            # å°è¯•è®¿é—®ä»˜è´¹ç‰ˆæ‰æœ‰çš„æ¨¡å‹
            paid_models = ["gemini-2.5-pro", "gemini-2.5-flash"]
            
            for model_name in paid_models:
                try:
                    model = genai.GenerativeModel(model_name)
                    # å°è¯•ä¸€ä¸ªæç®€çš„è¯·æ±‚
                    response = model.generate_content(
                        "1", 
                        generation_config=genai.types.GenerationConfig(
                            max_output_tokens=1,
                            temperature=0
                        )
                    )
                    # å¦‚æœæˆåŠŸï¼Œè¯´æ˜æ˜¯ä»˜è´¹ç‰ˆ
                    return True
                except Exception:
                    continue
            
            return False
            
        except Exception as e:
            logger.debug(f"æ£€æŸ¥ä»˜è´¹ç‰ˆå¤±è´¥: {e}")
            return False
    
    def _save_checkpoint(self) -> None:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨æ–‡ä»¶ç®¡ç†å™¨ä¿å­˜æ£€æŸ¥ç‚¹
        # æš‚æ—¶åªè®°å½•æ—¥å¿—
        logger.debug("Checkpoint saved")
    
    def _all_queries_processed(self, queries: List[str]) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æŸ¥è¯¢éƒ½å·²å¤„ç†
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            
        Returns:
            æ˜¯å¦å…¨éƒ¨å¤„ç†å®Œæˆ
        """
        for query in queries:
            normalized = self.scanner.normalize_query(query)
            if normalized not in self.scanner.filter.processed_queries:
                return False
        return True
    
    def _log_loop_stats(self, loop_count: int) -> None:
        """
        è®°å½•å¾ªç¯ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            loop_count: å¾ªç¯è®¡æ•°
        """
        logger.info(
            f"ğŸ Loop #{loop_count} complete - "
            f"Processed: {self.stats.total_items_processed} items | "
            f"Valid keys: {self.stats.total_valid_keys} | "
            f"Paid keys: {len(self.paid_keys_found)} | "
            f"Rate limited: {self.stats.total_rate_limited_keys}"
        )
    
    def _log_final_stats(self) -> None:
        """è®°å½•æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        logger.info("=" * 60)
        logger.info("ğŸ“Š FINAL STATISTICS")
        logger.info("=" * 60)
        logger.info(f"Total queries processed: {self.stats.total_queries_processed}")
        logger.info(f"Total items processed: {self.stats.total_items_processed}")
        logger.info(f"Total keys found: {self.stats.total_keys_found}")
        logger.info(f"Total valid keys: {self.stats.total_valid_keys}")
        logger.info(f"Total paid keys: {len(self.paid_keys_found)}")
        logger.info(f"Total rate limited keys: {self.stats.total_rate_limited_keys}")
        logger.info(f"Total errors: {self.stats.total_errors}")
        logger.info(f"Elapsed time: {self.stats.elapsed_time:.2f} seconds")
        logger.info(f"Processing rate: {self.stats.processing_rate:.2f} items/second")
        logger.info("=" * 60)
        
        # ä¿å­˜æ‰€æœ‰æ‰¾åˆ°çš„å¯†é’¥åˆ°æ±‡æ€»æ–‡ä»¶
        self._save_all_keys_summary()
    
    def _save_all_keys_summary(self) -> None:
        """ä¿å­˜æ‰€æœ‰æ‰¾åˆ°çš„å¯†é’¥æ±‡æ€»"""
        try:
            if not self.valid_keys_found and not self.rate_limited_keys_found:
                return
            
            # ç¡®ä¿dataç›®å½•å­˜åœ¨
            data_dir = Path("data")
            data_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ±‡æ€»æ–‡ä»¶
            summary_file = data_dir / f"gemini_keys_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(f"# Gemini APIå¯†é’¥æ±‡æ€»\n")
                f.write(f"# æ‰«ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# æ€»è®¡: {len(self.valid_keys_found)} ä¸ªæœ‰æ•ˆå¯†é’¥, {len(self.paid_keys_found)} ä¸ªä»˜è´¹ç‰ˆ, {len(self.rate_limited_keys_found)} ä¸ªé™æµå¯†é’¥\n\n")
                
                if self.paid_keys_found:
                    f.write("## ğŸ’ ä»˜è´¹ç‰ˆå¯†é’¥\n")
                    for key in sorted(self.paid_keys_found):
                        f.write(f"{key}\n")
                    f.write("\n")
                
                if self.valid_keys_found:
                    f.write("## âœ… æ‰€æœ‰æœ‰æ•ˆå¯†é’¥\n")
                    for key in sorted(self.valid_keys_found):
                        is_paid = " [ä»˜è´¹ç‰ˆ]" if key in self.paid_keys_found else ""
                        f.write(f"{key}{is_paid}\n")
                    f.write("\n")
                
                if self.rate_limited_keys_found:
                    f.write("## âš ï¸ é™æµå¯†é’¥ï¼ˆå¯èƒ½æœ‰æ•ˆï¼‰\n")
                    for key in sorted(self.rate_limited_keys_found):
                        f.write(f"{key}\n")
            
            logger.info(f"ğŸ’¾ å¯†é’¥æ±‡æ€»å·²ä¿å­˜åˆ°: {summary_file}")
            logger.info(f"   ğŸ’ ä»˜è´¹ç‰ˆå¯†é’¥: {len(self.paid_keys_found)} ä¸ª")
            logger.info(f"   âœ… æœ‰æ•ˆå¯†é’¥: {len(self.valid_keys_found)} ä¸ª")
            logger.info(f"   âš ï¸ é™æµå¯†é’¥: {len(self.rate_limited_keys_found)} ä¸ª")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¯†é’¥æ±‡æ€»å¤±è´¥: {e}")
    
    def stop(self) -> None:
        """åœæ­¢åè°ƒå™¨"""
        self.running = False
        logger.info("ğŸ›‘ Orchestrator stop requested")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.to_dict()