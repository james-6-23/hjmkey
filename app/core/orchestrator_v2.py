"""
åè°ƒå™¨ V2 - é›†æˆæ‰€æœ‰ä¼˜åŒ–ç»„ä»¶çš„å¢å¼ºç‰ˆ
åŒ…å«ç»Ÿä¸€ç»Ÿè®¡ã€å®‰å…¨æœºåˆ¶ã€ä¼˜é›…åœæœºã€åŸå­å†™å…¥ã€TokenPoolç­‰
"""

import asyncio
import time
import logging
import sys
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# å¯¼å…¥æ–°ç»„ä»¶
from app.core.stats import RunStats, KeyStatus, StatsManager
from app.core.graceful_shutdown import (
    GracefulShutdownManager, 
    OrchestratorState, 
    StateMachine
)
from app.core.scanner import Scanner, ScanResult
from app.core.validator import KeyValidator
from utils.file_utils import PathManager, AtomicFileWriter, RunArtifactManager
from utils.security_utils import (
    mask_key, 
    SecureKeyStorage, 
    setup_secure_logging,
    validate_environment
)
from utils.token_pool import TokenPool, TokenSelectionStrategy
from utils.github_client import GitHubClient
from app.services.config_service import get_config_service

logger = logging.getLogger(__name__)


class OrchestratorV2:
    """
    å¢å¼ºç‰ˆåè°ƒå™¨ - é›†æˆæ‰€æœ‰ä¼˜åŒ–
    """
    
    def __init__(self, scanner: Scanner = None, validator: KeyValidator = None):
        """
        åˆå§‹åŒ–åè°ƒå™¨ V2
        
        Args:
            scanner: æ‰«æå™¨å®ä¾‹
            validator: éªŒè¯å™¨å®ä¾‹
        """
        # è®¾ç½®å®‰å…¨æ—¥å¿—
        setup_secure_logging()
        validate_environment()
        
        # åˆå§‹åŒ–è·¯å¾„ç®¡ç†å™¨
        self.path_manager = PathManager()
        self.run_id = self.path_manager.set_run_id()
        
        logger.info("=" * 60)
        logger.info("ğŸš€ HAJIMI KING V2.0 - ENHANCED ORCHESTRATOR")
        logger.info(f"ğŸ“ Run ID: {self.run_id}")
        logger.info(f"ğŸ“‚ Run Directory: {self.path_manager.current_run_dir}")
        logger.info("=" * 60)
        
        # åˆå§‹åŒ–ç»Ÿè®¡ç®¡ç†å™¨
        self.stats_manager = StatsManager(self.path_manager.data_root)
        self.stats = self.stats_manager.create_run()
        
        # åˆå§‹åŒ–çŠ¶æ€æœºå’Œåœæœºç®¡ç†å™¨
        self.state_machine = StateMachine(OrchestratorState.IDLE)
        self.shutdown_manager = GracefulShutdownManager(self.state_machine)
        
        # æ³¨å†Œåœæœºå›è°ƒ
        self.shutdown_manager.register_cleanup(self._cleanup_resources)
        self.shutdown_manager.register_finalize(self._finalize_run)
        
        # åˆå§‹åŒ–æ–‡ä»¶ç®¡ç†å™¨
        self.writer = AtomicFileWriter()
        self.artifact_manager = RunArtifactManager(self.path_manager)
        
        # åˆå§‹åŒ–å®‰å…¨å­˜å‚¨
        self.secure_storage = SecureKeyStorage(
            self.path_manager.current_run_dir,
            allow_plaintext=get_config_service().get("ALLOW_PLAINTEXT", False)
        )
        
        # åˆå§‹åŒ–æ‰«æå™¨å’ŒéªŒè¯å™¨
        self.scanner = scanner or Scanner()
        self.validator = validator or KeyValidator()
        
        # åˆå§‹åŒ– GitHub å®¢æˆ·ç«¯å’Œ TokenPool
        self._init_github_client()
        
        # çº¿ç¨‹æ± ï¼ˆæ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´ï¼‰
        import multiprocessing
        max_workers = min(multiprocessing.cpu_count() * 2, 20)
        self._executor = ThreadPoolExecutor(max_workers=max_workers)
        logger.info(f"ğŸ”§ Thread pool initialized with {max_workers} workers")
        
        # è¿è¡Œæ ‡å¿—
        self.running = False
        
        logger.info("âœ… Orchestrator V2 initialized successfully")
    
    def _init_github_client(self):
        """åˆå§‹åŒ– GitHub å®¢æˆ·ç«¯å’Œ TokenPool"""
        config_service = get_config_service()
        tokens = config_service.get("GITHUB_TOKENS_LIST", [])
        
        if not tokens:
            logger.warning("âš ï¸ No GitHub tokens available")
            self.github_client = None
            self.token_pool = None
            return
        
        # åˆ›å»º TokenPool
        strategy = config_service.get("TOKEN_POOL_STRATEGY", "ADAPTIVE")
        strategy_enum = TokenSelectionStrategy[strategy.upper()]
        self.token_pool = TokenPool(tokens, strategy=strategy_enum)
        
        # åˆ›å»ºå¢å¼ºçš„ GitHub å®¢æˆ·ç«¯ï¼ˆç¨åå®ç°ï¼‰
        self.github_client = GitHubClient(tokens)
        
        logger.info(f"âœ… GitHub client initialized with {len(tokens)} tokens")
        logger.info(f"   Token pool strategy: {strategy}")
    
    async def run(self, queries: List[str], max_loops: Optional[int] = None) -> RunStats:
        """
        è¿è¡Œä¸»åè°ƒæµç¨‹
        
        Args:
            queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
            max_loops: æœ€å¤§å¾ªç¯æ¬¡æ•°
            
        Returns:
            è¿è¡Œç»Ÿè®¡
        """
        try:
            # ä½¿ç”¨åœæœºç®¡ç†å™¨çš„ä¸Šä¸‹æ–‡
            with self.shutdown_manager.managed_execution():
                # è½¬æ¢åˆ°åˆå§‹åŒ–çŠ¶æ€
                self.state_machine.transition_to(OrchestratorState.INITIALIZING)
                
                # è®¾ç½®è¿è¡Œå‚æ•°
                self.running = True
                self.stats.queries_planned = len(queries)
                
                # ä¿å­˜åˆå§‹æ£€æŸ¥ç‚¹
                self._save_checkpoint("initial")
                
                # è½¬æ¢åˆ°æ‰«æçŠ¶æ€
                self.state_machine.transition_to(OrchestratorState.SCANNING)
                
                # ä¸»å¾ªç¯
                loop_count = 0
                while self.running and (max_loops is None or loop_count < max_loops):
                    loop_count += 1
                    
                    # æ£€æŸ¥åœæœºè¯·æ±‚
                    if self.shutdown_manager.is_shutdown_requested():
                        logger.info("ğŸ›‘ Shutdown requested, stopping...")
                        break
                    
                    logger.info(f"ğŸ”„ Loop #{loop_count} - {datetime.now().strftime('%H:%M:%S')}")
                    
                    # å¤„ç†æŸ¥è¯¢
                    await self._process_queries(queries)
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    if self._all_queries_processed(queries):
                        logger.info("âœ… All queries processed")
                        break
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    self._save_checkpoint(f"loop_{loop_count}")
                    
                    # å¾ªç¯é—´å»¶è¿Ÿ
                    if self.running and (max_loops is None or loop_count < max_loops):
                        await asyncio.sleep(10)
                
                # è½¬æ¢åˆ°æœ€ç»ˆåŒ–çŠ¶æ€
                self.state_machine.transition_to(OrchestratorState.FINALIZING)
                
        except Exception as e:
            logger.error(f"ğŸ’¥ Orchestrator error: {e}")
            self.stats.add_error(type(e).__name__, str(e))
            self.state_machine.transition_to(OrchestratorState.ERROR)
            raise
        
        finally:
            self.running = False
            
        return self.stats
    
    async def _process_queries(self, queries: List[str]):
        """å¹¶å‘å¤„ç†æŸ¥è¯¢åˆ—è¡¨"""
        # è·å–é…ç½®
        config = get_config_service()
        max_concurrent = config.get("MAX_CONCURRENT_SEARCHES", 5)
        
        # è¿‡æ»¤æœªå¤„ç†çš„æŸ¥è¯¢
        pending_queries = []
        for query in queries:
            if self.scanner.should_skip_query(query):
                logger.info(f"â­ï¸ Skipping processed query: {query}")
            else:
                pending_queries.append(query)
        
        if not pending_queries:
            logger.info("âœ… All queries already processed")
            return
        
        logger.info(f"ğŸš€ Processing {len(pending_queries)} queries with {max_concurrent} concurrent workers")
        
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
        tasks = []
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_with_semaphore(query):
            """ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘"""
            async with semaphore:
                if not self.running or self.shutdown_manager.is_shutdown_requested():
                    return
                
                try:
                    await self._process_single_query(query)
                    self.stats.mark_query_complete(success=True)
                except Exception as e:
                    logger.error(f"âŒ Query failed: {query} - {e}")
                    self.stats.mark_query_complete(success=False)
                    self.stats.add_error("query_error", str(e), {"query": query})
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        for query in pending_queries:
            task = asyncio.create_task(process_with_semaphore(query))
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _process_single_query(self, query: str):
        """å¤„ç†å•ä¸ªæŸ¥è¯¢"""
        logger.info(f"ğŸ” Processing query: {query}")
        query_start_time = time.time()
        
        if not self.github_client or not self.token_pool:
            logger.error("âŒ GitHub client not initialized")
            return
        
        # è®°å½•æŸ¥è¯¢å¼€å§‹æ—¶çš„ç»Ÿè®¡
        start_stats = {
            'valid_free': self.stats.by_status[KeyStatus.VALID_FREE],
            'valid_paid': self.stats.by_status[KeyStatus.VALID_PAID],
            'rate_limited': self.stats.by_status[KeyStatus.RATE_LIMITED],
            'invalid': self.stats.by_status[KeyStatus.INVALID]
        }
        
        # ä» TokenPool è·å–ä»¤ç‰Œ
        token = self.token_pool.select_token()
        if not token:
            logger.error("âŒ No available tokens")
            return
        
        # ä½¿ç”¨è„±æ•æ—¥å¿—
        logger.info(f"ğŸ”‘ Using token: {mask_key(token)}")
        
        # æ‰§è¡Œæœç´¢ï¼ˆè¿™é‡Œéœ€è¦ä¿®æ”¹ GitHubClient æ¥æ”¯æŒå•ä¸ª tokenï¼‰
        # æš‚æ—¶ä½¿ç”¨åŸæœ‰æ–¹å¼
        start_time = time.time()
        search_result = self.github_client.search_for_keys(query)
        response_time = time.time() - start_time
        
        # æ›´æ–° TokenPool çŠ¶æ€
        self.token_pool.update_token_status(token, {
            'status_code': 200 if search_result else 500,
            'headers': {},  # éœ€è¦ä» GitHubClient è·å–
            'response_time': response_time
        })
        
        if not search_result or not search_result.get("items"):
            logger.info(f"ğŸ“­ No items found for query: {query}")
            return
        
        items = search_result["items"]
        logger.info(f"ğŸ“¦ Found {len(items)} items")
        
        # è½¬æ¢åˆ°éªŒè¯çŠ¶æ€
        self.state_machine.transition_to(OrchestratorState.VALIDATING)
        
        # å¤„ç†é¡¹ç›®
        for item in items:
            if not self.running or self.shutdown_manager.is_shutdown_requested():
                break
            
            await self._process_item(item)
        
        # æ ‡è®°æŸ¥è¯¢å®Œæˆ
        self.scanner.filter.add_processed_query(query)
        
        # è½¬å›æ‰«æçŠ¶æ€
        self.state_machine.transition_to(OrchestratorState.SCANNING)
        
        # æ˜¾ç¤ºæŸ¥è¯¢å®Œæˆåçš„ç»Ÿè®¡
        self._log_query_summary(query, start_stats, time.time() - query_start_time)
    
    async def _process_item(self, item: Dict[str, Any]):
        """å¤„ç†å•ä¸ªé¡¹ç›®"""
        # ä½¿ç”¨æ‰«æå™¨å¤„ç†
        result = self.scanner.process_search_item(item)
        
        if result.skipped_items > 0:
            return
        
        # è·å–æ–‡ä»¶å†…å®¹
        content = self.github_client.get_file_content(item)
        if not content:
            return
        
        # æå–å¯†é’¥
        keys = self.scanner.extract_keys_from_content(content)
        if not keys:
            return
        
        logger.info(f"ğŸ”‘ Found {len(keys)} suspected keys")
        
        # æ‰¹é‡å¹¶å‘éªŒè¯å¯†é’¥
        validation_results = await self._validate_keys_concurrent(keys)
        
        for val_result in validation_results:
            # ä½¿ç”¨è„±æ•æ—¥å¿—
            masked_key = mask_key(val_result.key)
            
            if val_result.is_valid:
                # åˆ¤æ–­æ˜¯å¦ä»˜è´¹
                is_paid = self._check_if_paid_key(val_result.key)
                status = KeyStatus.VALID_PAID if is_paid else KeyStatus.VALID_FREE
                
                self.stats.mark_key(val_result.key, status)
                logger.info(f"âœ… VALID ({status.name}): {masked_key}")
                
                # å®æ—¶ä¿å­˜æœ‰æ•ˆå¯†é’¥åˆ°æ–‡ä»¶
                self._save_key_to_file(val_result.key, status)
                
            elif val_result.is_rate_limited:
                self.stats.mark_key(val_result.key, KeyStatus.RATE_LIMITED)
                logger.warning(f"âš ï¸ RATE LIMITED: {masked_key}")
                
                # å®æ—¶ä¿å­˜é™æµå¯†é’¥åˆ°æ–‡ä»¶
                self._save_key_to_file(val_result.key, KeyStatus.RATE_LIMITED)
                
            else:
                self.stats.mark_key(val_result.key, KeyStatus.INVALID)
                logger.info(f"âŒ INVALID: {masked_key}")
                
                # å®æ—¶ä¿å­˜æ— æ•ˆå¯†é’¥åˆ°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
                self._save_key_to_file(val_result.key, KeyStatus.INVALID)
        
        # æ›´æ–°å¤„ç†ç»Ÿè®¡
        self.stats.items_processed += 1
    
    def _check_if_paid_key(self, key: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºä»˜è´¹å¯†é’¥"""
        try:
            import google.generativeai as genai
            genai.configure(api_key=key)
            
            # å°è¯•è®¿é—®ä»˜è´¹æ¨¡å‹
            paid_models = ["gemini-1.5-pro", "gemini-1.5-flash"]
            for model_name in paid_models:
                try:
                    model = genai.GenerativeModel(model_name)
                    response = model.generate_content(
                        "1",
                        generation_config=genai.types.GenerationConfig(
                            max_output_tokens=1,
                            temperature=0
                        )
                    )
                    return True
                except:
                    continue
            return False
        except:
            return False
    
    def _all_queries_processed(self, queries: List[str]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æŸ¥è¯¢éƒ½å·²å¤„ç†"""
        for query in queries:
            normalized = self.scanner.normalize_query(query)
            if normalized not in self.scanner.filter.processed_queries:
                return False
        return True
    
    def _save_checkpoint(self, label: str = ""):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            # è·å–token poolçŠ¶æ€ï¼Œä½†éœ€è¦å¤„ç†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
            token_pool_status = None
            if self.token_pool:
                pool_status = self.token_pool.get_pool_status()
                # è½¬æ¢strategy_usageä¸­çš„æšä¸¾ä¸ºå­—ç¬¦ä¸²
                if 'strategy_usage' in pool_status:
                    pool_status['strategy_usage'] = {
                        str(k): v for k, v in pool_status['strategy_usage'].items()
                    }
                token_pool_status = pool_status
            
            checkpoint_data = {
                "label": label,
                "timestamp": datetime.now().isoformat(),
                "run_id": self.run_id,
                "state": self.state_machine.state.name,
                "stats": self.stats.summary(),
                "processed_queries": list(self.scanner.filter.processed_queries),
                "token_pool_status": token_pool_status
            }
            
            self.artifact_manager.save_checkpoint(checkpoint_data)
            logger.debug(f"ğŸ’¾ Checkpoint saved: {label}")
            
        except Exception as e:
            logger.error(f"Failed to save checkpoint: {e}")
    
    def _cleanup_resources(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ Cleaning up resources...")
        
        # å…³é—­çº¿ç¨‹æ± 
        if hasattr(self, '_executor'):
            self._executor.shutdown(wait=False)
        
        # ä¿å­˜ TokenPool çŠ¶æ€
        if self.token_pool:
            pool_status = self.token_pool.get_pool_status()
            self.artifact_manager.save_artifact("token_pool_final.json", pool_status)
    
    def _finalize_run(self):
        """æœ€ç»ˆåŒ–è¿è¡Œ"""
        logger.info("ğŸ“ Finalizing run...")
        
        # å®Œæˆç»Ÿè®¡
        self.stats.finalize()
        
        # ä¿å­˜å¯†é’¥ï¼ˆå®‰å…¨å­˜å‚¨ï¼‰
        keys_by_status = {
            status.name: self.stats.get_keys_list(status)
            for status in KeyStatus
        }
        self.secure_storage.save_keys(keys_by_status)
        self.secure_storage.save_masked_summary(keys_by_status)
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        self.artifact_manager.save_final_report(self.stats.summary())
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        self._log_final_stats()
    
    def _log_final_stats(self):
        """è®°å½•æœ€ç»ˆç»Ÿè®¡"""
        summary = self.stats.summary()
        
        logger.info("=" * 60)
        logger.info("ğŸ“Š FINAL STATISTICS")
        logger.info("=" * 60)
        logger.info(f"Run ID: {summary['run_id']}")
        logger.info(f"Duration: {summary['duration_seconds']:.1f} seconds")
        logger.info(f"Queries: {summary['queries']['completed']}/{summary['queries']['planned']}")
        logger.info(f"Items processed: {summary['processing']['items_processed']}")
        logger.info(f"Valid keys (total/free/paid): {summary['keys']['valid_total']}/{summary['keys']['valid_free']}/{summary['keys']['valid_paid']}")
        logger.info(f"Rate limited: {summary['keys']['rate_limited']}")
        logger.info(f"Invalid: {summary['keys']['invalid']}")
        logger.info(f"Data loss: {summary['data_quality']['data_loss_ratio']}")
        logger.info(f"Errors: {summary['errors']['total']}")
        logger.info("=" * 60)
        
        # TokenPool ç»Ÿè®¡
        if self.token_pool:
            pool_status = self.token_pool.get_pool_status()
            logger.info("ğŸ“Š TOKEN POOL STATISTICS")
            logger.info(f"Total tokens: {pool_status['total_tokens']}")
            logger.info(f"Healthy/Limited/Exhausted: {pool_status['healthy']}/{pool_status['limited']}/{pool_status['exhausted']}")
            logger.info(f"Utilization: {pool_status['utilization']}")
            logger.info("=" * 60)


    def _save_key_to_file(self, key: str, status: KeyStatus):
        """å®æ—¶ä¿å­˜å¯†é’¥åˆ°æ–‡ä»¶"""
        try:
            # ç¡®å®šæ–‡ä»¶è·¯å¾„
            if status == KeyStatus.VALID_FREE:
                filename = "keys_valid_free.txt"
            elif status == KeyStatus.VALID_PAID:
                filename = "keys_valid_paid.txt"
            elif status == KeyStatus.RATE_LIMITED:
                filename = "keys_rate_limited.txt"
            elif status == KeyStatus.INVALID:
                filename = "keys_invalid.txt"
            else:
                return
            
            # æ„å»ºå®Œæ•´è·¯å¾„
            file_path = self.path_manager.current_run_dir / "secrets" / filename
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            # è¿½åŠ å†™å…¥å¯†é’¥
            with open(file_path, 'a', encoding='utf-8') as f:
                f.write(f"{key}\n")
            
            logger.debug(f"ğŸ’¾ Key saved to {filename}")
            
        except Exception as e:
            logger.error(f"Failed to save key to file: {e}")
    
    def _log_query_summary(self, query: str, start_stats: Dict, duration: float):
        """è®°å½•æŸ¥è¯¢å®Œæˆåçš„æ‘˜è¦"""
        # è®¡ç®—æ–°å¢çš„å¯†é’¥
        new_valid_free = self.stats.by_status[KeyStatus.VALID_FREE] - start_stats['valid_free']
        new_valid_paid = self.stats.by_status[KeyStatus.VALID_PAID] - start_stats['valid_paid']
        new_rate_limited = self.stats.by_status[KeyStatus.RATE_LIMITED] - start_stats['rate_limited']
        new_invalid = self.stats.by_status[KeyStatus.INVALID] - start_stats['invalid']
        
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š QUERY SUMMARY: {query[:50]}...")
        logger.info("=" * 60)
        logger.info(f"â±ï¸  Duration: {duration:.1f} seconds")
        logger.info(f"ğŸ”‘ Keys found in this query:")
        logger.info(f"   Valid (Free): +{new_valid_free}")
        logger.info(f"   Valid (Paid): +{new_valid_paid}")
        logger.info(f"   Rate Limited: +{new_rate_limited}")
        logger.info(f"   Invalid: +{new_invalid}")
        logger.info(f"ğŸ“ˆ Total keys so far:")
        logger.info(f"   Valid (Free): {self.stats.by_status[KeyStatus.VALID_FREE]}")
        logger.info(f"   Valid (Paid): {self.stats.by_status[KeyStatus.VALID_PAID]}")
        logger.info(f"   Rate Limited: {self.stats.by_status[KeyStatus.RATE_LIMITED]}")
        logger.info(f"   Invalid: {self.stats.by_status[KeyStatus.INVALID]}")
        
        # Token Pool çŠ¶æ€
        if self.token_pool:
            pool_status = self.token_pool.get_pool_status()
            logger.info(f"ğŸ¯ Token Pool Status:")
            logger.info(f"   Total tokens: {pool_status['total_tokens']}")
            logger.info(f"   Healthy: {pool_status['healthy']}")
            logger.info(f"   Limited: {pool_status['limited']}")
            logger.info(f"   Exhausted: {pool_status['exhausted']}")
            logger.info(f"   Quota remaining: {pool_status['total_remaining']}/{pool_status['total_limit']}")
            logger.info(f"   Utilization: {pool_status['utilization']}")
        
        logger.info("=" * 60)
    
    async def _validate_keys_concurrent(self, keys: List[str]) -> List[Any]:
        """å¹¶å‘éªŒè¯å¯†é’¥"""
        config = get_config_service()
        max_concurrent = config.get("ASYNC_VALIDATION_CONCURRENCY", 50)
        
        # å¦‚æœå¯†é’¥æ•°é‡è¾ƒå°‘ï¼Œç›´æ¥ä¸²è¡ŒéªŒè¯
        if len(keys) <= 5:
            return self.validator.validate_batch(keys)
        
        logger.info(f"âš¡ Validating {len(keys)} keys with {min(max_concurrent, len(keys))} concurrent workers")
        
        # åˆ›å»ºéªŒè¯ä»»åŠ¡
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def validate_with_limit(key):
            """é™åˆ¶å¹¶å‘çš„éªŒè¯"""
            async with semaphore:
                # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥éªŒè¯
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(
                    self._executor,
                    self.validator.validate_single,
                    key
                )
        
        # åˆ›å»ºæ‰€æœ‰éªŒè¯ä»»åŠ¡
        tasks = [validate_with_limit(key) for key in keys]
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰éªŒè¯
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤æ‰å¼‚å¸¸ç»“æœ
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Validation error for key {i}: {result}")
            else:
                valid_results.append(result)
        
        return valid_results


async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # åˆå§‹åŒ–åè°ƒå™¨
    orchestrator = OrchestratorV2()
    
    # æµ‹è¯•æŸ¥è¯¢
    queries = [
        "AIzaSy in:file",
        "AIzaSy in:file filename:.env",
        "AIzaSy in:file filename:config"
    ]
    
    # è¿è¡Œ
    stats = await orchestrator.run(queries, max_loops=1)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nâœ… Run completed: {stats.run_id}")
    print(f"   Valid keys found: {stats.by_status[KeyStatus.VALID_FREE] + stats.by_status[KeyStatus.VALID_PAID]}")


if __name__ == "__main__":
    import asyncio
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)s | %(name)s | %(message)s'
    )
    
    # è¿è¡Œ
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Program interrupted")