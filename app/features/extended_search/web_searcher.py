"""
Webå¹³å°Tokenæœç´¢å™¨
æœç´¢Stack Overflowã€Pastebinã€GitHub Gistç­‰å¹³å°çš„æ³„éœ²tokens
"""

import re
import requests
import time
import logging
from typing import List, Set, Dict, Any, Optional
from urllib.parse import quote

logger = logging.getLogger(__name__)


class WebSearcher:
    """Webå¹³å°Tokenæœç´¢å™¨"""
    
    # Tokenæ­£åˆ™æ¨¡å¼
    TOKEN_PATTERNS = [
        re.compile(r'ghp_[a-zA-Z0-9]{36}'),  # GitHub Personal Access Token
        re.compile(r'github_pat_[a-zA-Z0-9]{22}_[a-zA-Z0-9]{59}'),  # GitHub Fine-grained PAT
        re.compile(r'ghs_[a-zA-Z0-9]{36}'),  # GitHub App Token
        re.compile(r'AIzaSy[a-zA-Z0-9_-]{33}'),  # Google API Key
        re.compile(r'sk-[a-zA-Z0-9]{48}'),  # OpenAI API Key
        re.compile(r'glpat-[a-zA-Z0-9]{20}'),  # GitLab Personal Access Token
    ]
    
    def __init__(self, proxy: Optional[Dict[str, str]] = None):
        """
        åˆå§‹åŒ–Webæœç´¢å™¨
        
        Args:
            proxy: ä»£ç†é…ç½®
        """
        self.proxy = proxy
        self.session = requests.Session()
        if proxy:
            self.session.proxies.update(proxy)
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        logger.info("âœ… Webæœç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def search_stackoverflow(self, query: str = "github token api key", max_results: int = 50) -> List[str]:
        """
        æœç´¢Stack Overflow
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ‰¾åˆ°çš„tokenåˆ—è¡¨
        """
        tokens = set()
        logger.info(f"ğŸ” æœç´¢Stack Overflow: {query}")
        
        try:
            # ä½¿ç”¨Stack Exchange API
            api_url = "https://api.stackexchange.com/2.3/search/advanced"
            
            # åˆ†é¡µæœç´¢
            page = 1
            while len(tokens) < max_results and page <= 5:  # æœ€å¤š5é¡µ
                params = {
                    'order': 'desc',
                    'sort': 'creation',
                    'q': query,
                    'site': 'stackoverflow',
                    'pagesize': 100,
                    'page': page,
                    'filter': 'withbody'  # åŒ…å«é—®é¢˜å’Œç­”æ¡ˆçš„å†…å®¹
                }
                
                response = self.session.get(api_url, params=params, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    items = data.get('items', [])
                    
                    if not items:
                        break
                    
                    for item in items:
                        # æœç´¢é—®é¢˜å†…å®¹
                        title = item.get('title', '')
                        body = item.get('body', '')
                        content = f"{title} {body}"
                        
                        # æå–tokens
                        question_tokens = self._extract_tokens_from_text(content)
                        tokens.update(question_tokens)
                        
                        # æœç´¢ç­”æ¡ˆ
                        if 'answers' in item:
                            for answer in item['answers']:
                                answer_body = answer.get('body', '')
                                answer_tokens = self._extract_tokens_from_text(answer_body)
                                tokens.update(answer_tokens)
                    
                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µé¢
                    if not data.get('has_more', False):
                        break
                    
                    page += 1
                    time.sleep(0.5)  # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
                else:
                    logger.warning(f"Stack Overflow APIè¿”å›é”™è¯¯: {response.status_code}")
                    break
        
        except Exception as e:
            logger.error(f"æœç´¢Stack Overflowå¤±è´¥: {e}")
        
        logger.info(f"âœ… Stack Overflowæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(tokens)} ä¸ªtokens")
        return list(tokens)[:max_results]
    
    def search_pastebin_recent(self, max_results: int = 20) -> List[str]:
        """
        æœç´¢Pastebinæœ€è¿‘çš„å…¬å¼€å†…å®¹
        
        Args:
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ‰¾åˆ°çš„tokenåˆ—è¡¨
        """
        tokens = set()
        logger.info("ğŸ” æœç´¢Pastebinæœ€è¿‘å†…å®¹")
        
        try:
            # Pastebin scraping API
            api_url = "https://scrape.pastebin.com/api_scraping.php"
            params = {
                'limit': min(max_results * 5, 250)  # è·å–æ›´å¤šä»¥æé«˜å‘½ä¸­ç‡
            }
            
            response = self.session.get(api_url, params=params, timeout=10)
            if response.status_code == 200:
                pastes = response.json()
                
                for paste in pastes[:max_results * 2]:  # æ£€æŸ¥æ›´å¤špaste
                    paste_key = paste.get('key')
                    if paste_key:
                        # è·å–pasteå†…å®¹
                        content_url = f"https://pastebin.com/raw/{paste_key}"
                        try:
                            content_response = self.session.get(content_url, timeout=5)
                            if content_response.status_code == 200:
                                # æå–tokens
                                paste_tokens = self._extract_tokens_from_text(content_response.text)
                                tokens.update(paste_tokens)
                                
                                if len(tokens) >= max_results:
                                    break
                        except:
                            continue
                        
                        # é¿å…è¢«å°IP
                        time.sleep(1)
            else:
                logger.warning(f"Pastebin APIè¿”å›é”™è¯¯: {response.status_code}")
        
        except Exception as e:
            logger.error(f"æœç´¢Pastebinå¤±è´¥: {e}")
        
        logger.info(f"âœ… Pastebinæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(tokens)} ä¸ªtokens")
        return list(tokens)[:max_results]
    
    def search_github_gist(self, query: str = "AIzaSy", max_results: int = 30) -> List[str]:
        """
        æœç´¢GitHub Gist
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ‰¾åˆ°çš„tokenåˆ—è¡¨
        """
        tokens = set()
        logger.info(f"ğŸ” æœç´¢GitHub Gist: {query}")
        
        try:
            # ä½¿ç”¨GitHubæœç´¢APIæœç´¢Gist
            api_url = "https://api.github.com/search/code"
            
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_query = f'{query} in:file'
            
            params = {
                'q': search_query,
                'per_page': min(max_results, 100),
                'sort': 'indexed',
                'order': 'desc'
            }
            
            headers = {
                'Accept': 'application/vnd.github.v3+json'
            }
            
            response = self.session.get(api_url, params=params, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                items = data.get('items', [])
                
                for item in items:
                    # åªå¤„ç†Gist
                    if 'gist.github.com' in item.get('html_url', ''):
                        # è·å–æ–‡ä»¶å†…å®¹
                        file_url = item.get('url')
                        if file_url:
                            try:
                                file_response = self.session.get(file_url, headers=headers, timeout=5)
                                if file_response.status_code == 200:
                                    file_data = file_response.json()
                                    content = file_data.get('content', '')
                                    
                                    if content:
                                        # Base64è§£ç 
                                        import base64
                                        try:
                                            decoded_content = base64.b64decode(content).decode('utf-8')
                                            gist_tokens = self._extract_tokens_from_text(decoded_content)
                                            tokens.update(gist_tokens)
                                        except:
                                            pass
                            except:
                                continue
                            
                            time.sleep(0.5)  # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
            else:
                logger.warning(f"GitHub APIè¿”å›é”™è¯¯: {response.status_code}")
        
        except Exception as e:
            logger.error(f"æœç´¢GitHub Gistå¤±è´¥: {e}")
        
        logger.info(f"âœ… GitHub Gistæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(tokens)} ä¸ªtokens")
        return list(tokens)[:max_results]
    
    def search_reddit(self, subreddit: str = "programming", max_results: int = 30) -> List[str]:
        """
        æœç´¢Reddit
        
        Args:
            subreddit: å­ç‰ˆå—åç§°
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ‰¾åˆ°çš„tokenåˆ—è¡¨
        """
        tokens = set()
        logger.info(f"ğŸ” æœç´¢Reddit r/{subreddit}")
        
        try:
            # Reddit JSON API
            url = f"https://www.reddit.com/r/{subreddit}/search.json"
            params = {
                'q': 'api key OR token OR "AIzaSy" OR "ghp_"',
                'sort': 'new',
                'limit': min(max_results * 2, 100),
                'restrict_sr': 'true'
            }
            
            headers = {
                'User-Agent': 'TokenHunter/1.0'
            }
            
            response = self.session.get(url, params=params, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                posts = data.get('data', {}).get('children', [])
                
                for post in posts:
                    post_data = post.get('data', {})
                    
                    # æœç´¢æ ‡é¢˜å’Œå†…å®¹
                    title = post_data.get('title', '')
                    selftext = post_data.get('selftext', '')
                    content = f"{title} {selftext}"
                    
                    # æå–tokens
                    post_tokens = self._extract_tokens_from_text(content)
                    tokens.update(post_tokens)
                    
                    # ä¹Ÿæœç´¢è¯„è®ºï¼ˆå¦‚æœæœ‰URLï¼‰
                    if post_data.get('num_comments', 0) > 0:
                        comments_url = f"https://www.reddit.com{post_data.get('permalink', '')}.json"
                        try:
                            comments_response = self.session.get(comments_url, headers=headers, timeout=5)
                            if comments_response.status_code == 200:
                                comments_data = comments_response.json()
                                # ç®€å•å¤„ç†ç¬¬ä¸€å±‚è¯„è®º
                                if len(comments_data) > 1:
                                    comments = comments_data[1].get('data', {}).get('children', [])
                                    for comment in comments[:10]:  # åªçœ‹å‰10æ¡è¯„è®º
                                        comment_body = comment.get('data', {}).get('body', '')
                                        comment_tokens = self._extract_tokens_from_text(comment_body)
                                        tokens.update(comment_tokens)
                        except:
                            pass
                    
                    time.sleep(0.5)  # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
            else:
                logger.warning(f"Reddit APIè¿”å›é”™è¯¯: {response.status_code}")
        
        except Exception as e:
            logger.error(f"æœç´¢Redditå¤±è´¥: {e}")
        
        logger.info(f"âœ… Redditæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(tokens)} ä¸ªtokens")
        return list(tokens)[:max_results]
    
    def _extract_tokens_from_text(self, text: str) -> Set[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–tokens
        
        Args:
            text: è¦æœç´¢çš„æ–‡æœ¬
            
        Returns:
            æ‰¾åˆ°çš„tokené›†åˆ
        """
        tokens = set()
        if not text:
            return tokens
        
        # ä½¿ç”¨æ‰€æœ‰tokenæ¨¡å¼è¿›è¡ŒåŒ¹é…
        for pattern in self.TOKEN_PATTERNS:
            matches = pattern.findall(text)
            tokens.update(matches)
        
        # è¿‡æ»¤æ˜æ˜¾çš„å ä½ç¬¦
        filtered_tokens = set()
        for token in tokens:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å ä½ç¬¦
            if not any(placeholder in token.lower() for placeholder in ['xxx', 'your', 'example', 'test', 'demo']):
                # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯é‡å¤å­—ç¬¦
                if len(set(token[10:])) > 5:  # å‰ç¼€åçš„éƒ¨åˆ†æœ‰è¶³å¤Ÿçš„å­—ç¬¦å¤šæ ·æ€§
                    filtered_tokens.add(token)
        
        return filtered_tokens
    
    def search_all_platforms(self, max_results_per_platform: int = 20) -> Dict[str, List[str]]:
        """
        æœç´¢æ‰€æœ‰å¹³å°
        
        Args:
            max_results_per_platform: æ¯ä¸ªå¹³å°çš„æœ€å¤§ç»“æœæ•°
            
        Returns:
            å¹³å°åˆ°tokenåˆ—è¡¨çš„æ˜ å°„
        """
        results = {}
        
        logger.info("ğŸ” å¼€å§‹æœç´¢æ‰€æœ‰Webå¹³å°...")
        
        # Stack Overflow
        logger.info("æœç´¢Stack Overflow...")
        results['stackoverflow'] = self.search_stackoverflow(max_results=max_results_per_platform)
        
        # Pastebin
        logger.info("æœç´¢Pastebin...")
        results['pastebin'] = self.search_pastebin_recent(max_results=max_results_per_platform)
        
        # GitHub Gist
        logger.info("æœç´¢GitHub Gist...")
        results['github_gist'] = self.search_github_gist(max_results=max_results_per_platform)
        
        # Reddit
        logger.info("æœç´¢Reddit...")
        results['reddit'] = self.search_reddit(max_results=max_results_per_platform)
        
        # ç»Ÿè®¡ç»“æœ
        total_tokens = sum(len(tokens) for tokens in results.values())
        logger.info(f"âœ… Webæœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {total_tokens} ä¸ªæ½œåœ¨tokens")
        
        for platform, tokens in results.items():
            logger.info(f"   {platform}: {len(tokens)} tokens")
        
        return results
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'session'):
            self.session.close()